{
  "hash": "5f441b4fff99d571ecc910c11ec50be5",
  "result": {
    "markdown": "---\ntitle: \"AI Local Foods Team Week Three Wrap Up\"\nauthor: \"Aaron C\"\ndate: \"2023-06-02\"\ncategories: [\"Week Three\", \"Local Foods Team\"]\n---\n\n## Current Project Objectives\n\nThe currents project objectives for this week was to\n\n-   Catch up on any additional training.\n\n-   Collect and find data on heirloom tomatoes, eggs, and bacon.\n\n-   Learning how to do web scraping in python through Datacamp.\n\n-   Building programs to do web scraping\n\n## Works in Progress\n\nAn excel sheet that contains a list of small businesses of Iowa grocers that we had to go through and find places that had the data we wanted.\n\n![](DataCollection.PNG)\n\n![](PrathamData.png)\n\nThese are some examples of what we were looking for\n\n![](TomatoesData.PNG){width=\"662\"}\n\n![](EggsData.PNG)\n\nAlong with some manual data scraping. We started work on some data scraping programs (spiders). This code block is a spider that I recently created. However, there are still some improvements that still need to be made.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging\n\nclass FreshThymeBaconSpider(scrapy.Spider):\n    name = 'Fresh Thyme Market Bacon Spider'\n\n    def start_requests( self ):\n        start_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage']\n        for url in start_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse)\n    \n    def cardsParse(self, response):\n        #Fail safe for links\n        try:\n            #grabs all cards from list and saves the link to follow\n            xpath = '//*[contains(@class,\"Listing\")]/div/a/@href'\n            listCards = response.xpath(xpath)\n            linklist.append(listCards.extract())\n            for url in listCards:\n                yield response.follow( url = url, callback = self.itemParse, meta={'link': url} )\n        except AttributeError:\n           pass\n    \n    def itemParse(self, response):\n        #xpaths to the name and price\n        nameXpath = '//*[contains(@class, \"PdpInfoTitle\")]/text()'\n        priceXpath = '//*[contains(@class, \"PdpMainPrice\")]/text()'\n        url = response.meta.get('link')\n        #Grabs the name and price from the xpaths and adds them to the bacon list\n        bacon.append({'bacon': response.xpath(nameXpath).extract(), 'price': response.xpath(priceXpath).extract()})\n\n# Start\nconfigure_logging()\nbacon = []\nlinklist = []\nprocess = CrawlerProcess()\nprocess.crawl(FreshThymeBaconSpider)\nprocess.start()\nprocess.stop()\nbaconFrame = pd.DataFrame(bacon)\nprint(baconFrame)\n```\n:::\n\n\nThis image shows an example output of data we were able to scrape.\n\n![](SpiderExample.PNG)\n\n## DSPG Questions\n\n-   Are there stores or market places that would be helpful for us to look into?\n\n-   Is anyone experienced in web scraping and if so there any advice that you have for us?\n\n",
    "supporting": [
      "Week3_WrapUp_files"
    ],
    "filters": [],
    "includes": {}
  }
}
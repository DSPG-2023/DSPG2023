[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Project Participants",
    "section": "",
    "text": "Program Leader\n\nChristopher J. Seeger, Professor, Landscape Architecture; Extension Specialist in Geospatial Technologies\n\nProject Advisers\n\nLisa Bates, Assistant Director, Community and Economic Development\nLiesl Eathington, Research Scientist III\nBailey Hanson, Extension Specialist in Geospatial Technologies\nRakesh Shah, Extension Specialist in Computer Science\n\nProgram Advisers\n\nAdisak Sukul, Ph.D. Associate Teaching Professor\nHeike Hoffman, Professor\nTodd Abraham, Research Scientist\n\nGraduate Fellows\n\nMorenike Atejioye, Community and Regional Planning\nHarun Çelik, History and Graduate GIS Certificate\nSwati Kumari, Management Information Systems\nMohammad Ahnaf Sadat, Industrial Engineering and Artificial Intelligence\n\nUndergraduate Interns\n\nAaron Case, Computer Science\nAlex Cory, Data Science\nAngelina Evans, Geography (University of Iowa)\nGavin Fisher, Computer Science and Data Science\nKailyn Hogan, Community and Regional Planning\nAaron Null, Linguistics and Data Science\nSrika Raja, Statistics and Data Science"
  },
  {
    "objectID": "allBlogs.html",
    "href": "allBlogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 7, 2023\n\n\nAI Local Foods Team Week Eight Wrap Up\n\n\nAI/Local Food Team\n\n\n\n\nJul 7, 2023\n\n\nGrocery Team Week Eight Wrap Up\n\n\nAlex Cory\n\n\n\n\nJul 6, 2023\n\n\nHousing Team Week Eight Wrap Up\n\n\nGavin D. Fisher\n\n\n\n\nJun 30, 2023\n\n\nAI Local Foods Team Week Seven Wrap Up\n\n\nAI/Local Food Team\n\n\n\n\nJun 30, 2023\n\n\nHousing Team Week Seven Wrap Up\n\n\nKailyn Hogan\n\n\n\n\nJun 29, 2023\n\n\nGrocery Team Week Seven Wrap Up\n\n\nSrika Raja\n\n\n\n\nJun 23, 2023\n\n\nHousing Team Week Six Wrap Up\n\n\nAngelina Evans\n\n\n\n\nJun 22, 2023\n\n\nGrocery Team Week Six Wrap Up\n\n\nAaron Null\n\n\n\n\nJun 15, 2023\n\n\nGrocery Team Week Five Wrap Up\n\n\nAlex Cory\n\n\n\n\nJun 15, 2023\n\n\nHousing Team Recap Week Five\n\n\nGavin D. Fisher\n\n\n\n\nJun 9, 2023\n\n\nHousing Team Week Four Wrap Up\n\n\nKailyn Hogan\n\n\n\n\nJun 8, 2023\n\n\nAI Local Foods Team Week Four Wrap Up\n\n\nPratham Jadhav\n\n\n\n\nJun 8, 2023\n\n\nGrocery Team Week Four Wrap Up\n\n\nSrika Raja\n\n\n\n\nJun 2, 2023\n\n\nAI Local Foods Team Week Three Wrap Up\n\n\nAaron Case\n\n\n\n\nJun 2, 2023\n\n\nGrocery Team Week Three Wrap Up\n\n\nAaron Null\n\n\n\n\nMay 31, 2023\n\n\nHousing Team Week Three Wrap Up\n\n\nAngelina Evans\n\n\n\n\nMay 25, 2023\n\n\nHousing Team Week Two Wrap Up\n\n\nGavin Fisher\n\n\n\n\nMay 24, 2023\n\n\nAI Local Foods Team Week Two Wrap Up\n\n\nDev Rokade\n\n\n\n\nMay 24, 2023\n\n\nGrocery Team Week Two Wrap Up\n\n\nAlex Cory\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog2023/WeekEight/Foods-Week-Eight/Foods-Week-Eight.html",
    "href": "blog2023/WeekEight/Foods-Week-Eight/Foods-Week-Eight.html",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply."
  },
  {
    "objectID": "blog2023/WeekEight/Foods-Week-Eight/Foods-Week-Eight.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "href": "blog2023/WeekEight/Foods-Week-Eight/Foods-Week-Eight.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply."
  },
  {
    "objectID": "blog2023/WeekEight/Foods-Week-Eight/Foods-Week-Eight.html#final-presentation-flow",
    "href": "blog2023/WeekEight/Foods-Week-Eight/Foods-Week-Eight.html#final-presentation-flow",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "Final Presentation Flow",
    "text": "Final Presentation Flow\n\n\n\n\n\nSpeaker\n\n\n\n\n\nTopic\n\n\n\n\n\nTime\n\n\n\n\n\n\n\n \nSwati\n\n\n\n\nIntroduction\n\nWhat the project is about\nWhat we plan to achieve\nWhy is this important\n\n\n\n\n\n \n6 - 8 mins\n\n\n\n\n\n\n \nAaron\n\n\n\n\nWeb Scrapping (Spiders)\n\nWith what we started\nHow we did scraping\nWhat we achieved at the end\nAny interesting stuff\n\n\n\n\n\n \n10 - 12 mins\n\n\n\n\n\n\n \nSwati\n\n\n\n\nData Analysis\n\nVisualization (What graph say)\nAnswering research questions\nWhat model we used (if any)\n\n\n\n\n\n \n8 - 10 mins\n\n\n\n\n\n\n \nSadat\n\n\n\n\nCrop flow optimization\n\nOutput\nHow we achieved\n\n\n\n\n\n \n10 - 12 mins\n\n\n\n\n\n\n \nSadat / Swati\n\n\n\n\nConclusion and Future Vision\n\n\n\n\n \n2 - 3 mins"
  },
  {
    "objectID": "blog2023/WeekEight/Grocery-Week-Eight/Grocery_Week_Eight.html",
    "href": "blog2023/WeekEight/Grocery-Week-Eight/Grocery_Week_Eight.html",
    "title": "Grocery Team Week Eight Wrap Up",
    "section": "",
    "text": "Finished automation on market area calculations\nFigured out quarter circle method for determining the market area, got a direction from lat long coordinates\nMade and improved visualizations for final product and presentation\n\nPlots from SalesGenie data to show revenue for grocery stores of different sizes and city classifications\nWorked on the map for final project\n\nStarted work on vignettes\nMade interactive data tables\nFinalized Shiny UI"
  },
  {
    "objectID": "blog2023/WeekEight/Housing-Week-Eight/House_Week_Eight.html",
    "href": "blog2023/WeekEight/Housing-Week-Eight/House_Week_Eight.html",
    "title": "Housing Team Week Eight Wrap Up",
    "section": "",
    "text": "Shortened blog this week.\n\n\n\nFinished editing demographic profile plots\nState, regional, and national context added to graphs\nAdded percentages to some of the graphs\n\n\n\n\n\n\nCreated house attribute quality dashboards\nMade a guide on how to create the maps\nPlan to make graphs on AI model accuracy vs actual house quality\n\n\n\n\n\n\nThanks to Aaron’s spider, Beacon images were added to the Independence database evaluation\nSHAP model works on our own datasets (Sadat fixed the code)\nPlan to have SHAP models for each attribute to visualize how well the models are preforming\n\n\n\n\n\n\n\nFinish demographic analysis for Iowa communities\nGraph AI confidence vs accuracy\nImplement SHAP models into program (and hopefully CAM too!)"
  },
  {
    "objectID": "blog2023/WeekEight/Housing-Week-Eight/House_Week_Eight.html#ai-housing-week-eight-recap",
    "href": "blog2023/WeekEight/Housing-Week-Eight/House_Week_Eight.html#ai-housing-week-eight-recap",
    "title": "Housing Team Week Eight Wrap Up",
    "section": "",
    "text": "Shortened blog this week.\n\n\n\nFinished editing demographic profile plots\nState, regional, and national context added to graphs\nAdded percentages to some of the graphs\n\n\n\n\n\n\nCreated house attribute quality dashboards\nMade a guide on how to create the maps\nPlan to make graphs on AI model accuracy vs actual house quality\n\n\n\n\n\n\nThanks to Aaron’s spider, Beacon images were added to the Independence database evaluation\nSHAP model works on our own datasets (Sadat fixed the code)\nPlan to have SHAP models for each attribute to visualize how well the models are preforming\n\n\n\n\n\n\n\nFinish demographic analysis for Iowa communities\nGraph AI confidence vs accuracy\nImplement SHAP models into program (and hopefully CAM too!)"
  },
  {
    "objectID": "blog2023/WeekFive/Grocery-Week-Five/Grocery_Week_Five.html",
    "href": "blog2023/WeekFive/Grocery-Week-Five/Grocery_Week_Five.html",
    "title": "Grocery Team Week Five Wrap Up",
    "section": "",
    "text": "Voronoi Diagrams are ways to visualize the regions on a map that are the closest to a given node.\n\n\nggplot(spatial_results, aes(lat, lng)) +\n  stat_voronoi(geom = \"path\") +\n  geom_point(mapping = aes(lat, lng))\n\n\n\n\nOld Method\n\n\n\nWe looked into the economic methods for evaluating market size. Two metrics we looked into were Reilly’s Law of Gravitation and Trade Area Capture. Reilly’s law is based on the idea people will tend to shop in areas with a higher population, and the model is based on a ratio between distance and population. Trade area capture uses historical data and income in areas to give a prediction of actual numbers of people who shop in an area."
  },
  {
    "objectID": "blog2023/WeekFive/Grocery-Week-Five/Grocery_Week_Five.html#determining-market-size",
    "href": "blog2023/WeekFive/Grocery-Week-Five/Grocery_Week_Five.html#determining-market-size",
    "title": "Grocery Team Week Five Wrap Up",
    "section": "",
    "text": "We looked into the economic methods for evaluating market size. Two metrics we looked into were Reilly’s Law of Gravitation and Trade Area Capture. Reilly’s law is based on the idea people will tend to shop in areas with a higher population, and the model is based on a ratio between distance and population. Trade area capture uses historical data and income in areas to give a prediction of actual numbers of people who shop in an area."
  },
  {
    "objectID": "blog2023/WeekFive/Housing-Week-Five/House_Week_Five.html",
    "href": "blog2023/WeekFive/Housing-Week-Five/House_Week_Five.html",
    "title": "Housing Team Recap Week Five",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels.\n\nThe new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\n\n\n\nOn top of the AI models we needed to start filling in other characteristics about the addresses which we have collected. Although there have been a few errors in duplicate images and incorrect addresses we were able to link what pictures we currently have from Google into CSV files for each city. We can continue to grab data from Zillow.com and start collecting on Realtor.com\n\n\n\n\nIn order to get a head start on spatial mapping which we will use as part of our end results demonstration, we took a look into Geospatial Mapping on Datacamp, got back into using the census data, and took a look at Kyle Walkers TidyCensus book (online free!).\nUsing the US census data we started to look at changes in Iowa population from 2000-2020. \n\nWe tested out QGIS by mapping Slater and New Hampton using the lat long information off of the Google API.\n\n\n\n\n\n\nWe were able to meet with many vendors to learn about their companies and introduce our program and project. There were also many talks about Cyber Security, GIS, and IT.\nPresentations we attended: Modernize Your ArcGIS Web AppBuilder Apps Using Experience Builder by Mitch Winiecki, Bullwall by Don McCraw, ESRI Hands on Learning Lab by Rick Zellmer, Modernizing Utility Operations with ArcGIS by Chase Fisher, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule"
  },
  {
    "objectID": "blog2023/WeekFive/Housing-Week-Five/House_Week_Five.html#week-five-ai-housing-team",
    "href": "blog2023/WeekFive/Housing-Week-Five/House_Week_Five.html#week-five-ai-housing-team",
    "title": "Housing Team Recap Week Five",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels.\n\nThe new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\n\n\n\nOn top of the AI models we needed to start filling in other characteristics about the addresses which we have collected. Although there have been a few errors in duplicate images and incorrect addresses we were able to link what pictures we currently have from Google into CSV files for each city. We can continue to grab data from Zillow.com and start collecting on Realtor.com\n\n\n\n\nIn order to get a head start on spatial mapping which we will use as part of our end results demonstration, we took a look into Geospatial Mapping on Datacamp, got back into using the census data, and took a look at Kyle Walkers TidyCensus book (online free!).\nUsing the US census data we started to look at changes in Iowa population from 2000-2020. \n\nWe tested out QGIS by mapping Slater and New Hampton using the lat long information off of the Google API.\n\n\n\n\n\n\nWe were able to meet with many vendors to learn about their companies and introduce our program and project. There were also many talks about Cyber Security, GIS, and IT.\nPresentations we attended: Modernize Your ArcGIS Web AppBuilder Apps Using Experience Builder by Mitch Winiecki, Bullwall by Don McCraw, ESRI Hands on Learning Lab by Rick Zellmer, Modernizing Utility Operations with ArcGIS by Chase Fisher, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule"
  },
  {
    "objectID": "blog2023/WeekFour/Foods-Week-Four/Foods-Week-Four.html",
    "href": "blog2023/WeekFour/Foods-Week-Four/Foods-Week-Four.html",
    "title": "AI Local Foods Team Week Four Wrap Up",
    "section": "",
    "text": "Dev’s Work\nMonday- Collecting data for the project and working on finding data sources for Heirloom Tomatoes.\nTuesday- Collected in person data at Slater to analyze the housing conditions and amenities there in the first half of the day. Second half was remote where I worked on compiling and cleaning the data.\nWednesday- Created a script which cleaned scraped prices from Fareway Market website for locations Ames, Fort Dodge, Davenport, Iowa City, New Hampton, Clear Lake, Sioux City, Shenandoah in Iowa.\nThursday- Worked to get more data output, discussed presentation and blog wrap ups with team.\n\n\n\nThe project aims to address the need for localized and up-to-date demand forecasting information for Iowa’s local food producers.\nBy combining data and utilizing AI, the project seeks to develop a prototype application that will provide valuable insights to aid producers in making informed decisions about pricing, crop planning, and value-added processing.\nOver the course of three years, the project will progressively build upon previous work, incorporating data from various sources such as historical sales, weather patterns, and market trends.\nThe ultimate goal is to create a user-friendly tool that empowers farmers with the best information available for improving their local food business and making it more sustainable.\n\n\nPratham’s work\nProblem Statement:\n\n“The primary issue at hand is the lack of reliable demand information for local food producers in Iowa, which poses significant challenges in setting optimal prices and planning their operations effectively.”\n\n“Iowa’s local food producers lack reliable demand information, hindering their pricing and planning decisions. Our project uses AI and data analysis to develop an app that helps farmers forecast demand by considering sales history, weather, and local events. We aim to empower farmers with better decision-making tools and enhance the availability of local food in Iowa.”\n\nIn response, our project employs cutting-edge artificial intelligence and advanced data analysis techniques to develop a sophisticated application.\nThis application serves as a valuable tool for farmers, enabling them to make accurate demand forecasts by considering critical factors such as historical sales data, weather patterns, and local events.\nBy equipping farmers with enhanced decision-making capabilities, our goal is to empower them and facilitate the wider availability of locally produced food across Iowa.\n\n\n\nAaron’s Work\nThis is the scraper for Fresh Thyme Market using scrapy python package\n\n\nfrom datetime import datetime\\\n\nimport pandas as pd\\\n\nimport scrapy\\\n\nfrom scrapy.crawler import CrawlerProcess\\\n\nfrom scrapy.utils.log import configure_logging\n\n\n\nclass FreshThymeSpider(scrapy.Spider):\\\n\nname = 'Fresh Thyme Market Spider'\n\ndef start_requests( self ):\\\n\n# Bacon Scraper part\\\n\nbacon_urls = \\['[https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage'](https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage%27 \"https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=bacon&take=48&f=category%3ahot+dogs%2c+bacon+%26+sausage%27\"),\\\n\n'[https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage'\\]](https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage%27%5D \"https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=bacon&take=48&f=category%3ahot+dogs%2c+bacon+%26+sausage%27]\")\\\n\nfor url in bacon_urls:\\\n\nyield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'bacon', 'url': url})\n\n\n\n#Egg Scraper part\\\n\negg_urls = \\['[https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Eggs&take=48&f=Category%3AEggs'](https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Eggs&take=48&f=Category%3AEggs%27 \"https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=eggs&take=48&f=category%3aeggs%27\"),\\\n\n'[https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Eggs&take=48&f=Category%3AEggs'\\]](https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Eggs&take=48&f=Category%3AEggs%27%5D \"https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=eggs&take=48&f=category%3aeggs%27]\")\\\n\nfor url in egg_urls:\\\n\nyield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'egg', 'url': url})\n\n\n\n#Heirloom Tomatoes part\\\n\ntomato_urls = \\['[https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=heirloom%20tomatoes'](https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=heirloom%20tomatoes%27 \"https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=heirloom%20tomatoes%27\"),\\\n\n'[https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=heirloom%20tomatoes'\\]](https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=heirloom%20tomatoes%27%5D \"https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=heirloom%20tomatoes%27]\")\n\n\n\nfor url in tomato_urls:\\\n\nyield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'tomato', 'url': url})\n\n\n\ndef cardsParse(self, response):\\\n\n#Failsafe for links\\\n\ntry:\\\n\n#grabs the store location\\\n\nstoreXpath = '//\\*\\[contains(\\@class,\"HeaderSubtitle\")\\]/text()'\\\n\nstore = response.xpath(storeXpath).extract_first()\\\n\n#grabs all cards from list and saves the link to follow\\\n\nxpath = '//\\*\\[contains(\\@class,\"Listing\")\\]/div/a/\\@href'\\\n\nlistCards = response.xpath(xpath)\\\n\nfor url in listCards:\\\n\nyield response.follow( url = url, callback = self.itemParse, meta={'store': store, 'type': response.meta.get('type'), 'url': response.meta.get('url')} )\\\n\nexcept AttributeError:\\\n\npass\\\n\\\n\ndef itemParse(self, response):\\\n\n#xpaths to extract\\\n\nnameXpath = '//\\*\\[contains(\\@class, \"PdpInfoTitle\")\\]/text()'\\\n\npriceXpath = '//\\*\\[contains(\\@class, \"PdpMainPrice\")\\]/text()'\\\n\nunitPriceXpath = '//\\*\\[contains(\\@class, \"PdpPreviousPrice\")\\]/text()'\\\n\nprevPriceXpath = '//\\*\\[contains(\\@class, \"PdpUnitPrice\")\\]/text()'\\\n\n#Adding the data to data frame\\\n\nitemType = response.meta.get('type')\\\n\nif(itemType == \"bacon\"):\\\n\nbaconFrame.loc\\[len(baconFrame)\\] = \\[response.xpath(nameXpath).extract_first(),\\\n\nresponse.xpath(priceXpath).extract_first(),\\\n\nresponse.xpath(unitPriceXpath).extract_first(),\\\n\nresponse.xpath(prevPriceXpath).extract_first(),\\\n\nresponse.meta.get('store'),\\\n\nresponse.meta.get('url')\\]\\\n\n        elif(itemType == \"egg\"):\\\n\n            eggFrame.loc\\[len(eggFrame)\\] = \\[response.xpath(nameXpath).extract_first(),\\\n\n                                           response.xpath(priceXpath).extract_first(), \\\n\n                                           response.xpath(prevPriceXpath).extract_first(), \\\n\n                                           response.meta.get('store'),\\\n\n                                           response.meta.get('url')\\]\\\n\n        elif(itemType == \"tomato\"):\\\n\n            tomatoFrame.loc\\[len(tomatoFrame)\\] = \\[response.xpath(nameXpath).extract_first(),\\\n\n                                                 response.xpath(priceXpath).extract_first(), \\\n\n                                                 response.xpath(prevPriceXpath).extract_first(), \\\n\n                                                 response.meta.get('store'),\\\n\n                                                 response.meta.get('url')\\]\n\n \n\n\\# Start\\\n\n#DEBUG Switch\\\n\nDEBUG = 0\n\n \n\n#Data frames\\\n\nbaconFrame = pd.DataFrame(columns=\\['Bacon', 'Current Price', 'Unit Price', 'Sale', 'Store Location', 'Url'\\])\\\n\neggFrame = pd.DataFrame(columns=\\['Egg', 'Current Price', 'Sale', 'Store Location', 'Url'\\])\\\n\ntomatoFrame = pd.DataFrame(columns=\\['Heirloom Tomato', 'Current Price', 'Sale', 'Store Location', 'Url'\\])\n\n \n\nif(DEBUG):\\\n\n    #To see the inner mechanics of the spider\\\n\n    configure_logging()\n\n \n\n#This is to start the spider\\\n\nprocess = CrawlerProcess()\\\n\nprocess.crawl(FreshThymeSpider)\\\n\nprocess.start()\\\n\nprocess.stop()\n\n \n\nif(DEBUG):\\\n\n    #To see the outputs\\\n\n    print(baconFrame)\\\n\n    print(eggFrame)\\\n\n    print(tomatoFrame)\n\n \n\n#Adds the date that the data was scraped\\\n\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))\\[:-8\\]\\\n\n#To CSV files\\\n\nbaconFrame.to_csv(currentDate + \"Fresh Thyme Bacon.csv\")\\\n\neggFrame.to_csv(currentDate + \"Fresh Thyme Egg.csv\")\\\n\ntomatoFrame.to_csv(currentDate + \"Fresh Thyme Heirloom Tomatoes.csv\")\\\n\n\\`\\`\\`\n\n \n\n\\\n\nThis is the scraper for Hyvee made using selenium python package\\\n\n\\`\\`\\`{python}\\\n\n#\\|eval=FALSE\\\n\n#Imports\\\n\nfrom datetime import datetime\\\n\nimport pandas as pd\\\n\n#Imports for Scraping\\\n\nfrom selenium import webdriver\\\n\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\\\n\nfrom webdriver_manager.firefox import GeckoDriverManager\\\n\nfrom selenium.common.exceptions import NoSuchElementException\\\n\nfrom selenium.common.exceptions import StaleElementReferenceException\\\n\nfrom selenium.webdriver.common.by import By\\\n\nfrom selenium.webdriver.support.ui import WebDriverWait\\\n\nfrom selenium.webdriver.support import expected_conditions as EC\\\n\nfrom os import path\\\n\nimport time\n\n \n\n\\\n\nclass HyveeSpider():\\\n\n    name = \"Hyvee Spider\"\\\n\n    baconFrame = pd.DataFrame(columns=\\['Bacon', 'Current Price', 'Sale', 'Weight', 'Url'\\])\\\n\n    eggFrame = pd.DataFrame(columns=\\['Egg', 'Current Price', 'Sale', 'Amount', 'Url'\\])\\\n\n    tomatoFrame = pd.DataFrame(columns=\\['Heirloom Tomato', 'Current Price', 'Sale', 'Weight', 'Url'\\])\\\n\\\n\n    def \\_\\_init\\_\\_(self):\\\n\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\\\n\n        self.baconUrls = \\['[https://www.hy-vee.com/aisles-online/p/11315/Hormel-Black-Label-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/11315/Hormel-Black-Label-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/11315/hormel-black-label-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/47128/Hormel-Black-Label-Fully-Cooked-Original-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/47128/Hormel-Black-Label-Fully-Cooked-Original-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/47128/hormel-black-label-fully-cooked-original-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/41626/Applegate-Naturals-Uncured-Sunday-Bacon-Hickory-Smoked'](https://www.hy-vee.com/aisles-online/p/41626/Applegate-Naturals-Uncured-Sunday-Bacon-Hickory-Smoked%27 \"https://www.hy-vee.com/aisles-online/p/41626/applegate-naturals-uncured-sunday-bacon-hickory-smoked%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/57278/HyVee-Double-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/57278/HyVee-Double-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/57278/hyvee-double-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2405550/Applegate-Naturals-No-Sugar-Uncured-Bacon-Hickory-Smoked'](https://www.hy-vee.com/aisles-online/p/2405550/Applegate-Naturals-No-Sugar-Uncured-Bacon-Hickory-Smoked%27 \"https://www.hy-vee.com/aisles-online/p/2405550/applegate-naturals-no-sugar-uncured-bacon-hickory-smoked%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/57279/HyVee-Sweet-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/57279/HyVee-Sweet-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/57279/hyvee-sweet-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/11366/Hormel-Black-Label-Original-Bacon'](https://www.hy-vee.com/aisles-online/p/11366/Hormel-Black-Label-Original-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/11366/hormel-black-label-original-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2455081/Jimmy-Dean-Premium-Hickory-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/2455081/Jimmy-Dean-Premium-Hickory-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2455081/jimmy-dean-premium-hickory-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/3595492/Farmland-Bacon-Double-Smoked-Double-Thick-Cut'](https://www.hy-vee.com/aisles-online/p/3595492/Farmland-Bacon-Double-Smoked-Double-Thick-Cut%27 \"https://www.hy-vee.com/aisles-online/p/3595492/farmland-bacon-double-smoked-double-thick-cut%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/47117/Hormel-Black-Label-Center-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/47117/Hormel-Black-Label-Center-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/47117/hormel-black-label-center-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/57277/HyVee-Center-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/57277/HyVee-Center-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/57277/hyvee-center-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2199424/Country-Smokehouse-Thick-Applewood-Slab-Bacon'](https://www.hy-vee.com/aisles-online/p/2199424/Country-Smokehouse-Thick-Applewood-Slab-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2199424/country-smokehouse-thick-applewood-slab-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/77228/Hormel-Black-Label-Original-Bacon'](https://www.hy-vee.com/aisles-online/p/77228/Hormel-Black-Label-Original-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/77228/hormel-black-label-original-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21239/Farmland-Naturally-Hickory-Smoked-Classic-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21239/Farmland-Naturally-Hickory-Smoked-Classic-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21239/farmland-naturally-hickory-smoked-classic-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2456254/Jimmy-Dean-Premium-Applewood-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/2456254/Jimmy-Dean-Premium-Applewood-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2456254/jimmy-dean-premium-applewood-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21240/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21240/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21240/farmland-naturally-hickory-smoked-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/47159/Hormel-Black-Label-Original-Bacon-4Pk'](https://www.hy-vee.com/aisles-online/p/47159/Hormel-Black-Label-Original-Bacon-4Pk%27 \"https://www.hy-vee.com/aisles-online/p/47159/hormel-black-label-original-bacon-4pk%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/50315/Oscar-Mayer-Naturally-Hardwood-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/50315/Oscar-Mayer-Naturally-Hardwood-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/50315/oscar-mayer-naturally-hardwood-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/50321/Oscar-Mayer-Center-Cut-Original-Bacon'](https://www.hy-vee.com/aisles-online/p/50321/Oscar-Mayer-Center-Cut-Original-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/50321/oscar-mayer-center-cut-original-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/50316/Oscar-Mayer-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/50316/Oscar-Mayer-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/50316/oscar-mayer-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2199421/Country-Smokehouse-Thick-Hickory-Smoked-Slab-Bacon'](https://www.hy-vee.com/aisles-online/p/2199421/Country-Smokehouse-Thick-Hickory-Smoked-Slab-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2199421/country-smokehouse-thick-hickory-smoked-slab-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/104466/Hickory-Country-Bacon'](https://www.hy-vee.com/aisles-online/p/104466/Hickory-Country-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/104466/hickory-country-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23975/HyVee-Hickory-House-Applewood-Naturally-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/23975/HyVee-Hickory-House-Applewood-Naturally-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23975/hyvee-hickory-house-applewood-naturally-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23949/HyVee-Sweet-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/23949/HyVee-Sweet-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23949/hyvee-sweet-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23963/HyVee-Fully-Cooked-Hickory-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/23963/HyVee-Fully-Cooked-Hickory-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23963/hyvee-fully-cooked-hickory-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/11173/Hormel-Black-Label-Applewood-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/11173/Hormel-Black-Label-Applewood-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/11173/hormel-black-label-applewood-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21317/Farmland-Naturally-Applewood-Smoked-Classic-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21317/Farmland-Naturally-Applewood-Smoked-Classic-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21317/farmland-naturally-applewood-smoked-classic-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21238/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon-Package'](https://www.hy-vee.com/aisles-online/p/21238/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon-Package%27 \"https://www.hy-vee.com/aisles-online/p/21238/farmland-naturally-hickory-smoked-thick-cut-bacon-package%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23948/HyVee-Lower-Sodium-Sweet-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/23948/HyVee-Lower-Sodium-Sweet-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23948/hyvee-lower-sodium-sweet-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/458259/Wright-Naturally-Hickory-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/458259/Wright-Naturally-Hickory-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/458259/wright-naturally-hickory-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/11384/Hormel-Natural-Choice-Uncured-Original-Bacon-12-oz'](https://www.hy-vee.com/aisles-online/p/11384/Hormel-Natural-Choice-Uncured-Original-Bacon-12-oz%27 \"https://www.hy-vee.com/aisles-online/p/11384/hormel-natural-choice-uncured-original-bacon-12-oz%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2476490/Jimmy-Dean-FC-Hickory-Bacon'](https://www.hy-vee.com/aisles-online/p/2476490/Jimmy-Dean-FC-Hickory-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2476490/jimmy-dean-fc-hickory-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1646677/Smithfield-Hometown-Original-Bacon'](https://www.hy-vee.com/aisles-online/p/1646677/Smithfield-Hometown-Original-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1646677/smithfield-hometown-original-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/53849/Farmland-Naturally-Hickory-Smoked-Lower-Sodium-Classic-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/53849/Farmland-Naturally-Hickory-Smoked-Lower-Sodium-Classic-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/53849/farmland-naturally-hickory-smoked-lower-sodium-classic-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/47121/Hormel-Black-Label-Maple-Bacon'](https://www.hy-vee.com/aisles-online/p/47121/Hormel-Black-Label-Maple-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/47121/hormel-black-label-maple-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/164627/Oscar-Mayer-Fully-Cooked-Original-Bacon-252-oz-Box'](https://www.hy-vee.com/aisles-online/p/164627/Oscar-Mayer-Fully-Cooked-Original-Bacon-252-oz-Box%27 \"https://www.hy-vee.com/aisles-online/p/164627/oscar-mayer-fully-cooked-original-bacon-252-oz-box%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23974/HyVee-Hickory-House-Hickory-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/23974/HyVee-Hickory-House-Hickory-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23974/hyvee-hickory-house-hickory-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/50319/Oscar-Mayer-Selects-Smoked-Uncured-Bacon'](https://www.hy-vee.com/aisles-online/p/50319/Oscar-Mayer-Selects-Smoked-Uncured-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/50319/oscar-mayer-selects-smoked-uncured-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2471760/Jimmy-Dean-FC-Applewood-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/2471760/Jimmy-Dean-FC-Applewood-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2471760/jimmy-dean-fc-applewood-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/16239/Oscar-Mayer-Center-Cut-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/16239/Oscar-Mayer-Center-Cut-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/16239/oscar-mayer-center-cut-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2214511/Hormel-Black-Label-Original-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/2214511/Hormel-Black-Label-Original-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2214511/hormel-black-label-original-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1008152/Wright-Naturally-Smoked-Applewood-Bacon'](https://www.hy-vee.com/aisles-online/p/1008152/Wright-Naturally-Smoked-Applewood-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1008152/wright-naturally-smoked-applewood-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1813260/Smithfield-Naturally-Hickory-Smoked-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/1813260/Smithfield-Naturally-Hickory-Smoked-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1813260/smithfield-naturally-hickory-smoked-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23976/HyVee-Hickory-House-Peppered-Naturally-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/23976/HyVee-Hickory-House-Peppered-Naturally-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23976/hyvee-hickory-house-peppered-naturally-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21320/Farmland-Naturally-Applewood-Smoked-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21320/Farmland-Naturally-Applewood-Smoked-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21320/farmland-naturally-applewood-smoked-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21253/Farmland-Naturally-Hickory-Smoked-Extra-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21253/Farmland-Naturally-Hickory-Smoked-Extra-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21253/farmland-naturally-hickory-smoked-extra-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1255920/Hormel-Black-Label-Cherrywood-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/1255920/Hormel-Black-Label-Cherrywood-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1255920/hormel-black-label-cherrywood-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/57304/HyVee-Blue-Ribbon-Maple-Naturally-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/57304/HyVee-Blue-Ribbon-Maple-Naturally-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/57304/hyvee-blue-ribbon-maple-naturally-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21252/Farmland-Naturally-Hickory-Smoked-30-Less-Fat-Center-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21252/Farmland-Naturally-Hickory-Smoked-30-Less-Fat-Center-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21252/farmland-naturally-hickory-smoked-30-less-fat-center-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2501872/Bourbon-And-Brown-Sugar-Slab-Bacon'](https://www.hy-vee.com/aisles-online/p/2501872/Bourbon-And-Brown-Sugar-Slab-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2501872/bourbon-and-brown-sugar-slab-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2516586/Hormel-Natural-ChoiceOriginal-Thick-Cut-Uncured-Bacon'](https://www.hy-vee.com/aisles-online/p/2516586/Hormel-Natural-ChoiceOriginal-Thick-Cut-Uncured-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2516586/hormel-natural-choiceoriginal-thick-cut-uncured-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21319/Farmland-Naturally-Hickory-Smoked-Double-Smoked-Classic-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21319/Farmland-Naturally-Hickory-Smoked-Double-Smoked-Classic-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21319/farmland-naturally-hickory-smoked-double-smoked-classic-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/317829/Des-Moines-Bacon-And-Meat-Company-Hardwood-Smoked-Uncured-Country-Bacon'](https://www.hy-vee.com/aisles-online/p/317829/Des-Moines-Bacon-And-Meat-Company-Hardwood-Smoked-Uncured-Country-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/317829/des-moines-bacon-and-meat-company-hardwood-smoked-uncured-country-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1255919/Hormel-Black-Label-Jalapeno-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/1255919/Hormel-Black-Label-Jalapeno-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1255919/hormel-black-label-jalapeno-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/3538865/Oscar-Mayer-Bacon-Thick-Cut-Applewood'](https://www.hy-vee.com/aisles-online/p/3538865/Oscar-Mayer-Bacon-Thick-Cut-Applewood%27 \"https://www.hy-vee.com/aisles-online/p/3538865/oscar-mayer-bacon-thick-cut-applewood%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/317830/Des-Moines-Bacon-And-Meat-Company-Applewood-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/317830/Des-Moines-Bacon-And-Meat-Company-Applewood-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/317830/des-moines-bacon-and-meat-company-applewood-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/3308731/Oscar-Mayer-Natural-Fully-Cooked-Uncured-Bacon'](https://www.hy-vee.com/aisles-online/p/3308731/Oscar-Mayer-Natural-Fully-Cooked-Uncured-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/3308731/oscar-mayer-natural-fully-cooked-uncured-bacon%27\")\\\n\n                     \\]\\\n\n        self.eggsUrls = \\['[https://www.hy-vee.com/aisles-online/p/57236/HyVee-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/57236/HyVee-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/57236/hyvee-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/23899/HyVee-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/23899/HyVee-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/23899/hyvee-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/715446/Farmers-Hen-House-Free-Range-Organic-Large-Brown-Grade-A-Eggs'](https://www.hy-vee.com/aisles-online/p/715446/Farmers-Hen-House-Free-Range-Organic-Large-Brown-Grade-A-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/715446/farmers-hen-house-free-range-organic-large-brown-grade-a-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/2849570/Thats-Smart-Large-Shell-Eggs'](https://www.hy-vee.com/aisles-online/p/2849570/Thats-Smart-Large-Shell-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/2849570/thats-smart-large-shell-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/31351/Farmers-Hen-House-Free-Range-Grade-A-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/31351/Farmers-Hen-House-Free-Range-Grade-A-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/31351/farmers-hen-house-free-range-grade-a-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/23900/HyVee-Grade-A-Extra-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/23900/HyVee-Grade-A-Extra-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/23900/hyvee-grade-a-extra-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/71297/Egglands-Best-Farm-Fresh-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/71297/Egglands-Best-Farm-Fresh-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/71297/egglands-best-farm-fresh-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/36345/Egglands-Best-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/36345/Egglands-Best-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/36345/egglands-best-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/3192325/HyVee-Free-Range-Large-Brown-Egg-Grade-A'](https://www.hy-vee.com/aisles-online/p/3192325/HyVee-Free-Range-Large-Brown-Egg-Grade-A%27 \"https://www.hy-vee.com/aisles-online/p/3192325/hyvee-free-range-large-brown-egg-grade-a%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/23903/HyVee-Grade-A-Jumbo-Eggs'](https://www.hy-vee.com/aisles-online/p/23903/HyVee-Grade-A-Jumbo-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/23903/hyvee-grade-a-jumbo-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/3192323/HyVee-Cage-Free-Large-Brown-Egg-Grade-A'](https://www.hy-vee.com/aisles-online/p/3192323/HyVee-Cage-Free-Large-Brown-Egg-Grade-A%27 \"https://www.hy-vee.com/aisles-online/p/3192323/hyvee-cage-free-large-brown-egg-grade-a%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/36346/Egglands-Best-Cage-Free-Brown-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/36346/Egglands-Best-Cage-Free-Brown-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/36346/egglands-best-cage-free-brown-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/3192322/HyVee-Cage-Free-Large-Brown-Egg-Grade-A'](https://www.hy-vee.com/aisles-online/p/3192322/HyVee-Cage-Free-Large-Brown-Egg-Grade-A%27 \"https://www.hy-vee.com/aisles-online/p/3192322/hyvee-cage-free-large-brown-egg-grade-a%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/858343/HyVee-Cage-Free-Omega3-Grade-A-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/858343/HyVee-Cage-Free-Omega3-Grade-A-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/858343/hyvee-cage-free-omega3-grade-a-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/1901565/Farmers-Hen-House-Pasture-Raised-Organic-Grade-A-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/1901565/Farmers-Hen-House-Pasture-Raised-Organic-Grade-A-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/1901565/farmers-hen-house-pasture-raised-organic-grade-a-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/60364/HyVee-HealthMarket-Organic-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/60364/HyVee-HealthMarket-Organic-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/60364/hyvee-healthmarket-organic-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/71298/Egglands-Best-Extra-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/71298/Egglands-Best-Extra-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/71298/egglands-best-extra-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/23902/HyVee-Grade-A-Extra-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/23902/HyVee-Grade-A-Extra-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/23902/hyvee-grade-a-extra-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/453006/Egglands-Best-XL-Eggs'](https://www.hy-vee.com/aisles-online/p/453006/Egglands-Best-XL-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/453006/egglands-best-xl-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/2668550/HyVee-One-Step-Pasture-Raised-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/2668550/HyVee-One-Step-Pasture-Raised-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/2668550/hyvee-one-step-pasture-raised-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/66622/Farmers-Hen-House-Jumbo-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/66622/Farmers-Hen-House-Jumbo-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/66622/farmers-hen-house-jumbo-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/3274825/Nellies-Eggs-Brown-Free-Range-Large'](https://www.hy-vee.com/aisles-online/p/3274825/Nellies-Eggs-Brown-Free-Range-Large%27 \"https://www.hy-vee.com/aisles-online/p/3274825/nellies-eggs-brown-free-range-large%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/57235/HyVee-Grade-A-Medium-Eggs'](https://www.hy-vee.com/aisles-online/p/57235/HyVee-Grade-A-Medium-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/57235/hyvee-grade-a-medium-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/2437128/Pete-And-Gerrys-Eggs-Organic-Brown-Free-Range-Large'](https://www.hy-vee.com/aisles-online/p/2437128/Pete-And-Gerrys-Eggs-Organic-Brown-Free-Range-Large%27 \"https://www.hy-vee.com/aisles-online/p/2437128/pete-and-gerrys-eggs-organic-brown-free-range-large%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/36347/Egglands-Best-Organic-Cage-Free-Grade-A-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/36347/Egglands-Best-Organic-Cage-Free-Grade-A-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/36347/egglands-best-organic-cage-free-grade-a-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/2698224/Nellies-Free-Range-Eggs-Large-Fresh-Brown-Grade-A'](https://www.hy-vee.com/aisles-online/p/2698224/Nellies-Free-Range-Eggs-Large-Fresh-Brown-Grade-A%27 \"https://www.hy-vee.com/aisles-online/p/2698224/nellies-free-range-eggs-large-fresh-brown-grade-a%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/57237/HyVee-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/57237/HyVee-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/57237/hyvee-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/190508/Farmers-Hen-House-Organic-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/190508/Farmers-Hen-House-Organic-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/190508/farmers-hen-house-organic-large-brown-eggs%27\")\\\n\n                   \\]\\\n\n        self.tomatoesUrls = \\['[https://www.hy-vee.com/aisles-online/p/37174/'\\]](https://www.hy-vee.com/aisles-online/p/37174/%27%5D \"https://www.hy-vee.com/aisles-online/p/37174/%27]\")\\\n\n        self.count = 0\n\n \n\n    def dataWait(self, xpath):\\\n\n        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\\\n\n        element = WebDriverWait(self.driver, 3, ignored_exceptions=ignored_exceptions).until(EC.visibility_of_element_located((By.XPATH, xpath))).text\\\n\n        return element\\\n\\\n\n    def dataWaitForAll(self, xpath):\\\n\n            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\\\n\n            elements = WebDriverWait(self.driver, 30, ignored_exceptions=ignored_exceptions).until(EC.visibility_of_all_elements_located((By.XPATH, xpath)))\\\n\n            return len(elements)\n\n \n\n    def restart(self):\\\n\n        self.driver.close()\\\n\n        self.driver.quit()\\\n\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n\n \n\n    def start_requests( self ):\\\n\n        attemps = 3\n\n \n\n        self.count = 0\\\n\n        for trying in range(attemps):\\\n\n            try:\\\n\n                self.requestBacon()\\\n\n                print(\"Bacon Finished\")\\\n\n                break\\\n\n            except:\\\n\n                print(\"Bacon Export Failed Recovering extraction and continueing\")\\\n\n                self.restart()        \\\n\n        self.count = 0\\\n\n        for trying in range(attemps):\\\n\n            try:\\\n\n                self.requestEgg()\\\n\n                print(\"Eggs Finished\")\\\n\n                break\\\n\n            except:\\\n\n                print(\"Eggs Export Failed. Recovering extraction and continueing\")\\\n\n                self.restart()  \\\n\\\n\n        self.count = 0\\\n\n        for trying in range(attemps):\\\n\n            try:\\\n\n                self.requestTomato()\\\n\n                print(\"Heirloom Tomatoes Successfully Finished\")\\\n\n                break\\\n\n            except:\\\n\n                print(\"Tomatoes Export Failed Recovering extraction and continueing\")\\\n\n                self.restart()\\\n\\\n\n        self.driver.close()\\\n\n        self.driver.quit()\n\n \n\n    def requestBacon( self ):\\\n\n        total = len(self.baconUrls)\\\n\n        while self.count \\&lt; total:\\\n\n            url = self.baconUrls\\[self.count\\]\\\n\n            self.driver.get(url)\\\n\n            time.sleep(1) \\# marionette Error Fix\\\n\n            pXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p'\\\n\n            nameXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/h1'\\\n\n            priceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[1\\]'\\\n\n            sale = self.dataWaitForAll(pXpath) #Ensures the page is loaded\\\n\n            name = self.dataWait(nameXpath) \\\n\n            price = self.dataWait(priceXpath)\\\n\n            if sale == 2:\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.baconFrame.loc\\[len(self.baconFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                \"False\",\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            elif sale == 3:\\\n\n                prevPriceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[3\\]'\\\n\n                prevPrice = self.dataWait(prevPriceXpath)\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.baconFrame.loc\\[len(self.baconFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                prevPrice,\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            else:\\\n\n                \\# Catch if there is anything missing elements\\\n\n                self.baconFrame.loc\\[len(self.baconFrame)\\] = \\[\"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   url\\]\\\n\n            self.count += 1\\\n\n            print(\"Bacon item added\", self.count,\" of \", total,\" :  \", name)\\\n\n        self.count = 0\n\n \n\n    def requestEgg(self): \\\n\n        total = len(self.eggsUrls)\\\n\n        while self.count \\&lt; total:\\\n\n            url = self.eggsUrls\\[self.count\\]\\\n\n            self.driver.get(url)\\\n\n            time.sleep(1) \\# marionette Error Fix\\\n\n            pXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p'\\\n\n            nameXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/h1'\\\n\n            priceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[1\\]'\\\n\n            sale = self.dataWaitForAll(pXpath) #Ensures the page is loaded\\\n\n            name = self.dataWait(nameXpath) \\\n\n            price = self.dataWait(priceXpath)\\\n\n            if sale == 2:\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.eggFrame.loc\\[len(self.eggFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                \"False\",\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            elif sale == 3:\\\n\n                prevPriceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[3\\]'\\\n\n                prevPrice = self.dataWait(prevPriceXpath)\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.eggFrame.loc\\[len(self.eggFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                prevPrice,\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            else:\\\n\n                \\# Catch if there is anything missing elements\\\n\n                self.eggFrame.loc\\[len(self.eggFrame)\\] = \\[\"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   url\\]\\\n\n            self.count += 1\\\n\n            print(\"Eggs item added\", self.count,\" of \", total,\" :  \", name)\\\n\n        self.count = 0\n\n \n\n    def requestTomato( self ):\\\n\n        total = len(self.tomatoesUrls)\\\n\n        while self.count \\&lt; total:\\\n\n            url = self.tomatoesUrls\\[self.count\\]\\\n\n            self.driver.get(url)\\\n\n            time.sleep(1) \\# marionette Error Fix\\\n\n            pXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p'\\\n\n            nameXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/h1'\\\n\n            priceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[1\\]'\\\n\n            sale = self.dataWaitForAll(pXpath) #Ensures the page is loaded\\\n\n            name = self.dataWait(nameXpath) \\\n\n            price = self.dataWait(priceXpath)\\\n\n            if sale == 2:\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.tomatoFrame.loc\\[len(self.tomatoFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                \"False\",\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            elif sale == 3:\\\n\n                prevPriceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[3\\]'\\\n\n                prevPrice = self.dataWait(prevPriceXpath)\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.tomatoFrame.loc\\[len(self.tomatoFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                prevPrice,\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            else:\\\n\n                \\# Catch if there is anything missing elements\\\n\n                self.tomatoFrame.loc\\[len(self.tomatoFrame)\\] = \\[\"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   url\\]\\\n\n            self.count += 1\\\n\n            print(\"Tomato item added\", self.count,\"of\", total, \": \", name)\\\n\n        self.count = 0\n\n \n\n\\# Start\\\n\nspider = HyveeSpider()\\\n\nspider.start_requests()\\\n\n#Adds the date that the data was scraped\\\n\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))\\[:-8\\]\\\n\n#To CSV files\\\n\nspider.baconFrame.to_csv(currentDate + \"Hyvee Bacon.csv\")\\\n\nspider.eggFrame.to_csv(currentDate + \"Hyvee Egg.csv\")\\\n\nspider.tomatoFrame.to_csv(currentDate + \"Hyvee Heirloom Tomatoes.csv\")\n\n\n\n\nWeek 4: (Current Week): Data Collection and Preprocessing \n\nDuring this week, the team is focused on collecting the necessary data from identified sources. The collected data will undergo preprocessing tasks such as cleaning, transforming, and formatting to ensure its suitability for analysis and modeling. \n\nWeek 5: (Next Week): Exploratory Data Analysis and Feature Engineering \n\nIn the upcoming week, the team will perform exploratory data analysis to gain insights into the collected data. Statistical techniques and visualization methods will be employed to understand the distribution, patterns, and relationships within the data. Feature engineering techniques will also be applied to extract relevant features and create new variables that enhance the predictive power of the model. \n\nWeek 6: Model Development - Building and Training the Machine Learning Model \n\nFollowing the exploratory data analysis, the team will move on to developing the machine learning model for demand forecasting. Suitable algorithms will be selected based on the problem’s nature and the available data. The chosen model(s) will be implemented and trained using the preprocessed data, with model parameters fine-tuned for optimal performance. \n\nWeek 7: Model Evaluation, Fine-tuning, and Validation \n\nDuring this phase, the trained models will be rigorously evaluated to measure their performance and predictive capabilities. Various evaluation metrics will be calculated to assess the model’s effectiveness. The team will further fine-tune the model, adjusting parameters or exploring ensemble techniques to improve its performance. Validation techniques, such as holdout sets or cross-validation, will be employed to ensure the model’s generalizability and reliability. \n\nWeek 8: Final Testing, Presentation Preparation, and Documentation \n\nIn the final week, the developed model will undergo thorough testing to ensure its stability and accuracy in predicting demand for local food products. The team will prepare a comprehensive presentation summarizing the project’s objectives, methodology, findings, and recommendations. Documentation of the project, including the data collection process, preprocessing steps, modeling techniques, and results, will also be completed. \n These future and next steps will allow the team to progress through the remaining weeks, effectively building and evaluating the machine learning model for demand forecasting in the local food industry. \nFuture scope:  \nthe future scope of the project lies in expanding the application’s capabilities, incorporating additional data sources, refining the machine learning models, and fostering collaboration within the local food ecosystem. By continually improving and adapting the solution, the project can contribute to the sustainable growth and success of Iowa’s local food producers."
  },
  {
    "objectID": "blog2023/WeekFour/Foods-Week-Four/Foods-Week-Four.html#project-overview",
    "href": "blog2023/WeekFour/Foods-Week-Four/Foods-Week-Four.html#project-overview",
    "title": "AI Local Foods Team Week Four Wrap Up",
    "section": "",
    "text": "The project aims to address the need for localized and up-to-date demand forecasting information for Iowa’s local food producers.\nBy combining data and utilizing AI, the project seeks to develop a prototype application that will provide valuable insights to aid producers in making informed decisions about pricing, crop planning, and value-added processing.\nOver the course of three years, the project will progressively build upon previous work, incorporating data from various sources such as historical sales, weather patterns, and market trends.\nThe ultimate goal is to create a user-friendly tool that empowers farmers with the best information available for improving their local food business and making it more sustainable.\n\n\nPratham’s work\nProblem Statement:\n\n“The primary issue at hand is the lack of reliable demand information for local food producers in Iowa, which poses significant challenges in setting optimal prices and planning their operations effectively.”\n\n“Iowa’s local food producers lack reliable demand information, hindering their pricing and planning decisions. Our project uses AI and data analysis to develop an app that helps farmers forecast demand by considering sales history, weather, and local events. We aim to empower farmers with better decision-making tools and enhance the availability of local food in Iowa.”\n\nIn response, our project employs cutting-edge artificial intelligence and advanced data analysis techniques to develop a sophisticated application.\nThis application serves as a valuable tool for farmers, enabling them to make accurate demand forecasts by considering critical factors such as historical sales data, weather patterns, and local events.\nBy equipping farmers with enhanced decision-making capabilities, our goal is to empower them and facilitate the wider availability of locally produced food across Iowa.\n\n\n\nAaron’s Work\nThis is the scraper for Fresh Thyme Market using scrapy python package\n\n\nfrom datetime import datetime\\\n\nimport pandas as pd\\\n\nimport scrapy\\\n\nfrom scrapy.crawler import CrawlerProcess\\\n\nfrom scrapy.utils.log import configure_logging\n\n\n\nclass FreshThymeSpider(scrapy.Spider):\\\n\nname = 'Fresh Thyme Market Spider'\n\ndef start_requests( self ):\\\n\n# Bacon Scraper part\\\n\nbacon_urls = \\['[https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage'](https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage%27 \"https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=bacon&take=48&f=category%3ahot+dogs%2c+bacon+%26+sausage%27\"),\\\n\n'[https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage'\\]](https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage%27%5D \"https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=bacon&take=48&f=category%3ahot+dogs%2c+bacon+%26+sausage%27]\")\\\n\nfor url in bacon_urls:\\\n\nyield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'bacon', 'url': url})\n\n\n\n#Egg Scraper part\\\n\negg_urls = \\['[https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Eggs&take=48&f=Category%3AEggs'](https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Eggs&take=48&f=Category%3AEggs%27 \"https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=eggs&take=48&f=category%3aeggs%27\"),\\\n\n'[https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Eggs&take=48&f=Category%3AEggs'\\]](https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Eggs&take=48&f=Category%3AEggs%27%5D \"https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=eggs&take=48&f=category%3aeggs%27]\")\\\n\nfor url in egg_urls:\\\n\nyield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'egg', 'url': url})\n\n\n\n#Heirloom Tomatoes part\\\n\ntomato_urls = \\['[https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=heirloom%20tomatoes'](https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=heirloom%20tomatoes%27 \"https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=heirloom%20tomatoes%27\"),\\\n\n'[https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=heirloom%20tomatoes'\\]](https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=heirloom%20tomatoes%27%5D \"https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=heirloom%20tomatoes%27]\")\n\n\n\nfor url in tomato_urls:\\\n\nyield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'tomato', 'url': url})\n\n\n\ndef cardsParse(self, response):\\\n\n#Failsafe for links\\\n\ntry:\\\n\n#grabs the store location\\\n\nstoreXpath = '//\\*\\[contains(\\@class,\"HeaderSubtitle\")\\]/text()'\\\n\nstore = response.xpath(storeXpath).extract_first()\\\n\n#grabs all cards from list and saves the link to follow\\\n\nxpath = '//\\*\\[contains(\\@class,\"Listing\")\\]/div/a/\\@href'\\\n\nlistCards = response.xpath(xpath)\\\n\nfor url in listCards:\\\n\nyield response.follow( url = url, callback = self.itemParse, meta={'store': store, 'type': response.meta.get('type'), 'url': response.meta.get('url')} )\\\n\nexcept AttributeError:\\\n\npass\\\n\\\n\ndef itemParse(self, response):\\\n\n#xpaths to extract\\\n\nnameXpath = '//\\*\\[contains(\\@class, \"PdpInfoTitle\")\\]/text()'\\\n\npriceXpath = '//\\*\\[contains(\\@class, \"PdpMainPrice\")\\]/text()'\\\n\nunitPriceXpath = '//\\*\\[contains(\\@class, \"PdpPreviousPrice\")\\]/text()'\\\n\nprevPriceXpath = '//\\*\\[contains(\\@class, \"PdpUnitPrice\")\\]/text()'\\\n\n#Adding the data to data frame\\\n\nitemType = response.meta.get('type')\\\n\nif(itemType == \"bacon\"):\\\n\nbaconFrame.loc\\[len(baconFrame)\\] = \\[response.xpath(nameXpath).extract_first(),\\\n\nresponse.xpath(priceXpath).extract_first(),\\\n\nresponse.xpath(unitPriceXpath).extract_first(),\\\n\nresponse.xpath(prevPriceXpath).extract_first(),\\\n\nresponse.meta.get('store'),\\\n\nresponse.meta.get('url')\\]\\\n\n        elif(itemType == \"egg\"):\\\n\n            eggFrame.loc\\[len(eggFrame)\\] = \\[response.xpath(nameXpath).extract_first(),\\\n\n                                           response.xpath(priceXpath).extract_first(), \\\n\n                                           response.xpath(prevPriceXpath).extract_first(), \\\n\n                                           response.meta.get('store'),\\\n\n                                           response.meta.get('url')\\]\\\n\n        elif(itemType == \"tomato\"):\\\n\n            tomatoFrame.loc\\[len(tomatoFrame)\\] = \\[response.xpath(nameXpath).extract_first(),\\\n\n                                                 response.xpath(priceXpath).extract_first(), \\\n\n                                                 response.xpath(prevPriceXpath).extract_first(), \\\n\n                                                 response.meta.get('store'),\\\n\n                                                 response.meta.get('url')\\]\n\n \n\n\\# Start\\\n\n#DEBUG Switch\\\n\nDEBUG = 0\n\n \n\n#Data frames\\\n\nbaconFrame = pd.DataFrame(columns=\\['Bacon', 'Current Price', 'Unit Price', 'Sale', 'Store Location', 'Url'\\])\\\n\neggFrame = pd.DataFrame(columns=\\['Egg', 'Current Price', 'Sale', 'Store Location', 'Url'\\])\\\n\ntomatoFrame = pd.DataFrame(columns=\\['Heirloom Tomato', 'Current Price', 'Sale', 'Store Location', 'Url'\\])\n\n \n\nif(DEBUG):\\\n\n    #To see the inner mechanics of the spider\\\n\n    configure_logging()\n\n \n\n#This is to start the spider\\\n\nprocess = CrawlerProcess()\\\n\nprocess.crawl(FreshThymeSpider)\\\n\nprocess.start()\\\n\nprocess.stop()\n\n \n\nif(DEBUG):\\\n\n    #To see the outputs\\\n\n    print(baconFrame)\\\n\n    print(eggFrame)\\\n\n    print(tomatoFrame)\n\n \n\n#Adds the date that the data was scraped\\\n\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))\\[:-8\\]\\\n\n#To CSV files\\\n\nbaconFrame.to_csv(currentDate + \"Fresh Thyme Bacon.csv\")\\\n\neggFrame.to_csv(currentDate + \"Fresh Thyme Egg.csv\")\\\n\ntomatoFrame.to_csv(currentDate + \"Fresh Thyme Heirloom Tomatoes.csv\")\\\n\n\\`\\`\\`\n\n \n\n\\\n\nThis is the scraper for Hyvee made using selenium python package\\\n\n\\`\\`\\`{python}\\\n\n#\\|eval=FALSE\\\n\n#Imports\\\n\nfrom datetime import datetime\\\n\nimport pandas as pd\\\n\n#Imports for Scraping\\\n\nfrom selenium import webdriver\\\n\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\\\n\nfrom webdriver_manager.firefox import GeckoDriverManager\\\n\nfrom selenium.common.exceptions import NoSuchElementException\\\n\nfrom selenium.common.exceptions import StaleElementReferenceException\\\n\nfrom selenium.webdriver.common.by import By\\\n\nfrom selenium.webdriver.support.ui import WebDriverWait\\\n\nfrom selenium.webdriver.support import expected_conditions as EC\\\n\nfrom os import path\\\n\nimport time\n\n \n\n\\\n\nclass HyveeSpider():\\\n\n    name = \"Hyvee Spider\"\\\n\n    baconFrame = pd.DataFrame(columns=\\['Bacon', 'Current Price', 'Sale', 'Weight', 'Url'\\])\\\n\n    eggFrame = pd.DataFrame(columns=\\['Egg', 'Current Price', 'Sale', 'Amount', 'Url'\\])\\\n\n    tomatoFrame = pd.DataFrame(columns=\\['Heirloom Tomato', 'Current Price', 'Sale', 'Weight', 'Url'\\])\\\n\\\n\n    def \\_\\_init\\_\\_(self):\\\n\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\\\n\n        self.baconUrls = \\['[https://www.hy-vee.com/aisles-online/p/11315/Hormel-Black-Label-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/11315/Hormel-Black-Label-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/11315/hormel-black-label-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/47128/Hormel-Black-Label-Fully-Cooked-Original-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/47128/Hormel-Black-Label-Fully-Cooked-Original-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/47128/hormel-black-label-fully-cooked-original-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/41626/Applegate-Naturals-Uncured-Sunday-Bacon-Hickory-Smoked'](https://www.hy-vee.com/aisles-online/p/41626/Applegate-Naturals-Uncured-Sunday-Bacon-Hickory-Smoked%27 \"https://www.hy-vee.com/aisles-online/p/41626/applegate-naturals-uncured-sunday-bacon-hickory-smoked%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/57278/HyVee-Double-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/57278/HyVee-Double-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/57278/hyvee-double-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2405550/Applegate-Naturals-No-Sugar-Uncured-Bacon-Hickory-Smoked'](https://www.hy-vee.com/aisles-online/p/2405550/Applegate-Naturals-No-Sugar-Uncured-Bacon-Hickory-Smoked%27 \"https://www.hy-vee.com/aisles-online/p/2405550/applegate-naturals-no-sugar-uncured-bacon-hickory-smoked%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/57279/HyVee-Sweet-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/57279/HyVee-Sweet-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/57279/hyvee-sweet-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/11366/Hormel-Black-Label-Original-Bacon'](https://www.hy-vee.com/aisles-online/p/11366/Hormel-Black-Label-Original-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/11366/hormel-black-label-original-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2455081/Jimmy-Dean-Premium-Hickory-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/2455081/Jimmy-Dean-Premium-Hickory-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2455081/jimmy-dean-premium-hickory-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/3595492/Farmland-Bacon-Double-Smoked-Double-Thick-Cut'](https://www.hy-vee.com/aisles-online/p/3595492/Farmland-Bacon-Double-Smoked-Double-Thick-Cut%27 \"https://www.hy-vee.com/aisles-online/p/3595492/farmland-bacon-double-smoked-double-thick-cut%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/47117/Hormel-Black-Label-Center-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/47117/Hormel-Black-Label-Center-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/47117/hormel-black-label-center-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/57277/HyVee-Center-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/57277/HyVee-Center-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/57277/hyvee-center-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2199424/Country-Smokehouse-Thick-Applewood-Slab-Bacon'](https://www.hy-vee.com/aisles-online/p/2199424/Country-Smokehouse-Thick-Applewood-Slab-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2199424/country-smokehouse-thick-applewood-slab-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/77228/Hormel-Black-Label-Original-Bacon'](https://www.hy-vee.com/aisles-online/p/77228/Hormel-Black-Label-Original-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/77228/hormel-black-label-original-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21239/Farmland-Naturally-Hickory-Smoked-Classic-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21239/Farmland-Naturally-Hickory-Smoked-Classic-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21239/farmland-naturally-hickory-smoked-classic-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2456254/Jimmy-Dean-Premium-Applewood-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/2456254/Jimmy-Dean-Premium-Applewood-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2456254/jimmy-dean-premium-applewood-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21240/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21240/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21240/farmland-naturally-hickory-smoked-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/47159/Hormel-Black-Label-Original-Bacon-4Pk'](https://www.hy-vee.com/aisles-online/p/47159/Hormel-Black-Label-Original-Bacon-4Pk%27 \"https://www.hy-vee.com/aisles-online/p/47159/hormel-black-label-original-bacon-4pk%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/50315/Oscar-Mayer-Naturally-Hardwood-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/50315/Oscar-Mayer-Naturally-Hardwood-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/50315/oscar-mayer-naturally-hardwood-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/50321/Oscar-Mayer-Center-Cut-Original-Bacon'](https://www.hy-vee.com/aisles-online/p/50321/Oscar-Mayer-Center-Cut-Original-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/50321/oscar-mayer-center-cut-original-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/50316/Oscar-Mayer-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/50316/Oscar-Mayer-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/50316/oscar-mayer-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2199421/Country-Smokehouse-Thick-Hickory-Smoked-Slab-Bacon'](https://www.hy-vee.com/aisles-online/p/2199421/Country-Smokehouse-Thick-Hickory-Smoked-Slab-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2199421/country-smokehouse-thick-hickory-smoked-slab-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/104466/Hickory-Country-Bacon'](https://www.hy-vee.com/aisles-online/p/104466/Hickory-Country-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/104466/hickory-country-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23975/HyVee-Hickory-House-Applewood-Naturally-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/23975/HyVee-Hickory-House-Applewood-Naturally-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23975/hyvee-hickory-house-applewood-naturally-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23949/HyVee-Sweet-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/23949/HyVee-Sweet-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23949/hyvee-sweet-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23963/HyVee-Fully-Cooked-Hickory-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/23963/HyVee-Fully-Cooked-Hickory-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23963/hyvee-fully-cooked-hickory-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/11173/Hormel-Black-Label-Applewood-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/11173/Hormel-Black-Label-Applewood-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/11173/hormel-black-label-applewood-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21317/Farmland-Naturally-Applewood-Smoked-Classic-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21317/Farmland-Naturally-Applewood-Smoked-Classic-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21317/farmland-naturally-applewood-smoked-classic-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21238/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon-Package'](https://www.hy-vee.com/aisles-online/p/21238/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon-Package%27 \"https://www.hy-vee.com/aisles-online/p/21238/farmland-naturally-hickory-smoked-thick-cut-bacon-package%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23948/HyVee-Lower-Sodium-Sweet-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/23948/HyVee-Lower-Sodium-Sweet-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23948/hyvee-lower-sodium-sweet-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/458259/Wright-Naturally-Hickory-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/458259/Wright-Naturally-Hickory-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/458259/wright-naturally-hickory-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/11384/Hormel-Natural-Choice-Uncured-Original-Bacon-12-oz'](https://www.hy-vee.com/aisles-online/p/11384/Hormel-Natural-Choice-Uncured-Original-Bacon-12-oz%27 \"https://www.hy-vee.com/aisles-online/p/11384/hormel-natural-choice-uncured-original-bacon-12-oz%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2476490/Jimmy-Dean-FC-Hickory-Bacon'](https://www.hy-vee.com/aisles-online/p/2476490/Jimmy-Dean-FC-Hickory-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2476490/jimmy-dean-fc-hickory-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1646677/Smithfield-Hometown-Original-Bacon'](https://www.hy-vee.com/aisles-online/p/1646677/Smithfield-Hometown-Original-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1646677/smithfield-hometown-original-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/53849/Farmland-Naturally-Hickory-Smoked-Lower-Sodium-Classic-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/53849/Farmland-Naturally-Hickory-Smoked-Lower-Sodium-Classic-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/53849/farmland-naturally-hickory-smoked-lower-sodium-classic-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/47121/Hormel-Black-Label-Maple-Bacon'](https://www.hy-vee.com/aisles-online/p/47121/Hormel-Black-Label-Maple-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/47121/hormel-black-label-maple-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/164627/Oscar-Mayer-Fully-Cooked-Original-Bacon-252-oz-Box'](https://www.hy-vee.com/aisles-online/p/164627/Oscar-Mayer-Fully-Cooked-Original-Bacon-252-oz-Box%27 \"https://www.hy-vee.com/aisles-online/p/164627/oscar-mayer-fully-cooked-original-bacon-252-oz-box%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23974/HyVee-Hickory-House-Hickory-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/23974/HyVee-Hickory-House-Hickory-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23974/hyvee-hickory-house-hickory-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/50319/Oscar-Mayer-Selects-Smoked-Uncured-Bacon'](https://www.hy-vee.com/aisles-online/p/50319/Oscar-Mayer-Selects-Smoked-Uncured-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/50319/oscar-mayer-selects-smoked-uncured-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2471760/Jimmy-Dean-FC-Applewood-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/2471760/Jimmy-Dean-FC-Applewood-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2471760/jimmy-dean-fc-applewood-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/16239/Oscar-Mayer-Center-Cut-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/16239/Oscar-Mayer-Center-Cut-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/16239/oscar-mayer-center-cut-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2214511/Hormel-Black-Label-Original-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/2214511/Hormel-Black-Label-Original-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2214511/hormel-black-label-original-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1008152/Wright-Naturally-Smoked-Applewood-Bacon'](https://www.hy-vee.com/aisles-online/p/1008152/Wright-Naturally-Smoked-Applewood-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1008152/wright-naturally-smoked-applewood-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1813260/Smithfield-Naturally-Hickory-Smoked-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/1813260/Smithfield-Naturally-Hickory-Smoked-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1813260/smithfield-naturally-hickory-smoked-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/23976/HyVee-Hickory-House-Peppered-Naturally-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/23976/HyVee-Hickory-House-Peppered-Naturally-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/23976/hyvee-hickory-house-peppered-naturally-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21320/Farmland-Naturally-Applewood-Smoked-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21320/Farmland-Naturally-Applewood-Smoked-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21320/farmland-naturally-applewood-smoked-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21253/Farmland-Naturally-Hickory-Smoked-Extra-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21253/Farmland-Naturally-Hickory-Smoked-Extra-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21253/farmland-naturally-hickory-smoked-extra-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1255920/Hormel-Black-Label-Cherrywood-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/1255920/Hormel-Black-Label-Cherrywood-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1255920/hormel-black-label-cherrywood-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/57304/HyVee-Blue-Ribbon-Maple-Naturally-Smoked-Thick-Sliced-Bacon'](https://www.hy-vee.com/aisles-online/p/57304/HyVee-Blue-Ribbon-Maple-Naturally-Smoked-Thick-Sliced-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/57304/hyvee-blue-ribbon-maple-naturally-smoked-thick-sliced-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21252/Farmland-Naturally-Hickory-Smoked-30-Less-Fat-Center-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21252/Farmland-Naturally-Hickory-Smoked-30-Less-Fat-Center-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21252/farmland-naturally-hickory-smoked-30-less-fat-center-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2501872/Bourbon-And-Brown-Sugar-Slab-Bacon'](https://www.hy-vee.com/aisles-online/p/2501872/Bourbon-And-Brown-Sugar-Slab-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2501872/bourbon-and-brown-sugar-slab-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/2516586/Hormel-Natural-ChoiceOriginal-Thick-Cut-Uncured-Bacon'](https://www.hy-vee.com/aisles-online/p/2516586/Hormel-Natural-ChoiceOriginal-Thick-Cut-Uncured-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/2516586/hormel-natural-choiceoriginal-thick-cut-uncured-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/21319/Farmland-Naturally-Hickory-Smoked-Double-Smoked-Classic-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/21319/Farmland-Naturally-Hickory-Smoked-Double-Smoked-Classic-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/21319/farmland-naturally-hickory-smoked-double-smoked-classic-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/317829/Des-Moines-Bacon-And-Meat-Company-Hardwood-Smoked-Uncured-Country-Bacon'](https://www.hy-vee.com/aisles-online/p/317829/Des-Moines-Bacon-And-Meat-Company-Hardwood-Smoked-Uncured-Country-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/317829/des-moines-bacon-and-meat-company-hardwood-smoked-uncured-country-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/1255919/Hormel-Black-Label-Jalapeno-Thick-Cut-Bacon'](https://www.hy-vee.com/aisles-online/p/1255919/Hormel-Black-Label-Jalapeno-Thick-Cut-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/1255919/hormel-black-label-jalapeno-thick-cut-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/3538865/Oscar-Mayer-Bacon-Thick-Cut-Applewood'](https://www.hy-vee.com/aisles-online/p/3538865/Oscar-Mayer-Bacon-Thick-Cut-Applewood%27 \"https://www.hy-vee.com/aisles-online/p/3538865/oscar-mayer-bacon-thick-cut-applewood%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/317830/Des-Moines-Bacon-And-Meat-Company-Applewood-Smoked-Bacon'](https://www.hy-vee.com/aisles-online/p/317830/Des-Moines-Bacon-And-Meat-Company-Applewood-Smoked-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/317830/des-moines-bacon-and-meat-company-applewood-smoked-bacon%27\"),\\\n\n                     '[https://www.hy-vee.com/aisles-online/p/3308731/Oscar-Mayer-Natural-Fully-Cooked-Uncured-Bacon'](https://www.hy-vee.com/aisles-online/p/3308731/Oscar-Mayer-Natural-Fully-Cooked-Uncured-Bacon%27 \"https://www.hy-vee.com/aisles-online/p/3308731/oscar-mayer-natural-fully-cooked-uncured-bacon%27\")\\\n\n                     \\]\\\n\n        self.eggsUrls = \\['[https://www.hy-vee.com/aisles-online/p/57236/HyVee-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/57236/HyVee-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/57236/hyvee-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/23899/HyVee-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/23899/HyVee-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/23899/hyvee-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/715446/Farmers-Hen-House-Free-Range-Organic-Large-Brown-Grade-A-Eggs'](https://www.hy-vee.com/aisles-online/p/715446/Farmers-Hen-House-Free-Range-Organic-Large-Brown-Grade-A-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/715446/farmers-hen-house-free-range-organic-large-brown-grade-a-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/2849570/Thats-Smart-Large-Shell-Eggs'](https://www.hy-vee.com/aisles-online/p/2849570/Thats-Smart-Large-Shell-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/2849570/thats-smart-large-shell-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/31351/Farmers-Hen-House-Free-Range-Grade-A-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/31351/Farmers-Hen-House-Free-Range-Grade-A-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/31351/farmers-hen-house-free-range-grade-a-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/23900/HyVee-Grade-A-Extra-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/23900/HyVee-Grade-A-Extra-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/23900/hyvee-grade-a-extra-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/71297/Egglands-Best-Farm-Fresh-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/71297/Egglands-Best-Farm-Fresh-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/71297/egglands-best-farm-fresh-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/36345/Egglands-Best-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/36345/Egglands-Best-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/36345/egglands-best-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/3192325/HyVee-Free-Range-Large-Brown-Egg-Grade-A'](https://www.hy-vee.com/aisles-online/p/3192325/HyVee-Free-Range-Large-Brown-Egg-Grade-A%27 \"https://www.hy-vee.com/aisles-online/p/3192325/hyvee-free-range-large-brown-egg-grade-a%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/23903/HyVee-Grade-A-Jumbo-Eggs'](https://www.hy-vee.com/aisles-online/p/23903/HyVee-Grade-A-Jumbo-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/23903/hyvee-grade-a-jumbo-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/3192323/HyVee-Cage-Free-Large-Brown-Egg-Grade-A'](https://www.hy-vee.com/aisles-online/p/3192323/HyVee-Cage-Free-Large-Brown-Egg-Grade-A%27 \"https://www.hy-vee.com/aisles-online/p/3192323/hyvee-cage-free-large-brown-egg-grade-a%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/36346/Egglands-Best-Cage-Free-Brown-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/36346/Egglands-Best-Cage-Free-Brown-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/36346/egglands-best-cage-free-brown-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/3192322/HyVee-Cage-Free-Large-Brown-Egg-Grade-A'](https://www.hy-vee.com/aisles-online/p/3192322/HyVee-Cage-Free-Large-Brown-Egg-Grade-A%27 \"https://www.hy-vee.com/aisles-online/p/3192322/hyvee-cage-free-large-brown-egg-grade-a%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/858343/HyVee-Cage-Free-Omega3-Grade-A-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/858343/HyVee-Cage-Free-Omega3-Grade-A-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/858343/hyvee-cage-free-omega3-grade-a-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/1901565/Farmers-Hen-House-Pasture-Raised-Organic-Grade-A-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/1901565/Farmers-Hen-House-Pasture-Raised-Organic-Grade-A-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/1901565/farmers-hen-house-pasture-raised-organic-grade-a-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/60364/HyVee-HealthMarket-Organic-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/60364/HyVee-HealthMarket-Organic-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/60364/hyvee-healthmarket-organic-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/71298/Egglands-Best-Extra-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/71298/Egglands-Best-Extra-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/71298/egglands-best-extra-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/23902/HyVee-Grade-A-Extra-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/23902/HyVee-Grade-A-Extra-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/23902/hyvee-grade-a-extra-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/453006/Egglands-Best-XL-Eggs'](https://www.hy-vee.com/aisles-online/p/453006/Egglands-Best-XL-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/453006/egglands-best-xl-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/2668550/HyVee-One-Step-Pasture-Raised-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/2668550/HyVee-One-Step-Pasture-Raised-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/2668550/hyvee-one-step-pasture-raised-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/66622/Farmers-Hen-House-Jumbo-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/66622/Farmers-Hen-House-Jumbo-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/66622/farmers-hen-house-jumbo-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/3274825/Nellies-Eggs-Brown-Free-Range-Large'](https://www.hy-vee.com/aisles-online/p/3274825/Nellies-Eggs-Brown-Free-Range-Large%27 \"https://www.hy-vee.com/aisles-online/p/3274825/nellies-eggs-brown-free-range-large%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/57235/HyVee-Grade-A-Medium-Eggs'](https://www.hy-vee.com/aisles-online/p/57235/HyVee-Grade-A-Medium-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/57235/hyvee-grade-a-medium-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/2437128/Pete-And-Gerrys-Eggs-Organic-Brown-Free-Range-Large'](https://www.hy-vee.com/aisles-online/p/2437128/Pete-And-Gerrys-Eggs-Organic-Brown-Free-Range-Large%27 \"https://www.hy-vee.com/aisles-online/p/2437128/pete-and-gerrys-eggs-organic-brown-free-range-large%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/36347/Egglands-Best-Organic-Cage-Free-Grade-A-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/36347/Egglands-Best-Organic-Cage-Free-Grade-A-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/36347/egglands-best-organic-cage-free-grade-a-large-brown-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/2698224/Nellies-Free-Range-Eggs-Large-Fresh-Brown-Grade-A'](https://www.hy-vee.com/aisles-online/p/2698224/Nellies-Free-Range-Eggs-Large-Fresh-Brown-Grade-A%27 \"https://www.hy-vee.com/aisles-online/p/2698224/nellies-free-range-eggs-large-fresh-brown-grade-a%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/57237/HyVee-Grade-A-Large-Eggs'](https://www.hy-vee.com/aisles-online/p/57237/HyVee-Grade-A-Large-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/57237/hyvee-grade-a-large-eggs%27\"),\\\n\n                    '[https://www.hy-vee.com/aisles-online/p/190508/Farmers-Hen-House-Organic-Large-Brown-Eggs'](https://www.hy-vee.com/aisles-online/p/190508/Farmers-Hen-House-Organic-Large-Brown-Eggs%27 \"https://www.hy-vee.com/aisles-online/p/190508/farmers-hen-house-organic-large-brown-eggs%27\")\\\n\n                   \\]\\\n\n        self.tomatoesUrls = \\['[https://www.hy-vee.com/aisles-online/p/37174/'\\]](https://www.hy-vee.com/aisles-online/p/37174/%27%5D \"https://www.hy-vee.com/aisles-online/p/37174/%27]\")\\\n\n        self.count = 0\n\n \n\n    def dataWait(self, xpath):\\\n\n        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\\\n\n        element = WebDriverWait(self.driver, 3, ignored_exceptions=ignored_exceptions).until(EC.visibility_of_element_located((By.XPATH, xpath))).text\\\n\n        return element\\\n\\\n\n    def dataWaitForAll(self, xpath):\\\n\n            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\\\n\n            elements = WebDriverWait(self.driver, 30, ignored_exceptions=ignored_exceptions).until(EC.visibility_of_all_elements_located((By.XPATH, xpath)))\\\n\n            return len(elements)\n\n \n\n    def restart(self):\\\n\n        self.driver.close()\\\n\n        self.driver.quit()\\\n\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n\n \n\n    def start_requests( self ):\\\n\n        attemps = 3\n\n \n\n        self.count = 0\\\n\n        for trying in range(attemps):\\\n\n            try:\\\n\n                self.requestBacon()\\\n\n                print(\"Bacon Finished\")\\\n\n                break\\\n\n            except:\\\n\n                print(\"Bacon Export Failed Recovering extraction and continueing\")\\\n\n                self.restart()        \\\n\n        self.count = 0\\\n\n        for trying in range(attemps):\\\n\n            try:\\\n\n                self.requestEgg()\\\n\n                print(\"Eggs Finished\")\\\n\n                break\\\n\n            except:\\\n\n                print(\"Eggs Export Failed. Recovering extraction and continueing\")\\\n\n                self.restart()  \\\n\\\n\n        self.count = 0\\\n\n        for trying in range(attemps):\\\n\n            try:\\\n\n                self.requestTomato()\\\n\n                print(\"Heirloom Tomatoes Successfully Finished\")\\\n\n                break\\\n\n            except:\\\n\n                print(\"Tomatoes Export Failed Recovering extraction and continueing\")\\\n\n                self.restart()\\\n\\\n\n        self.driver.close()\\\n\n        self.driver.quit()\n\n \n\n    def requestBacon( self ):\\\n\n        total = len(self.baconUrls)\\\n\n        while self.count \\&lt; total:\\\n\n            url = self.baconUrls\\[self.count\\]\\\n\n            self.driver.get(url)\\\n\n            time.sleep(1) \\# marionette Error Fix\\\n\n            pXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p'\\\n\n            nameXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/h1'\\\n\n            priceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[1\\]'\\\n\n            sale = self.dataWaitForAll(pXpath) #Ensures the page is loaded\\\n\n            name = self.dataWait(nameXpath) \\\n\n            price = self.dataWait(priceXpath)\\\n\n            if sale == 2:\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.baconFrame.loc\\[len(self.baconFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                \"False\",\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            elif sale == 3:\\\n\n                prevPriceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[3\\]'\\\n\n                prevPrice = self.dataWait(prevPriceXpath)\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.baconFrame.loc\\[len(self.baconFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                prevPrice,\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            else:\\\n\n                \\# Catch if there is anything missing elements\\\n\n                self.baconFrame.loc\\[len(self.baconFrame)\\] = \\[\"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   url\\]\\\n\n            self.count += 1\\\n\n            print(\"Bacon item added\", self.count,\" of \", total,\" :  \", name)\\\n\n        self.count = 0\n\n \n\n    def requestEgg(self): \\\n\n        total = len(self.eggsUrls)\\\n\n        while self.count \\&lt; total:\\\n\n            url = self.eggsUrls\\[self.count\\]\\\n\n            self.driver.get(url)\\\n\n            time.sleep(1) \\# marionette Error Fix\\\n\n            pXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p'\\\n\n            nameXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/h1'\\\n\n            priceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[1\\]'\\\n\n            sale = self.dataWaitForAll(pXpath) #Ensures the page is loaded\\\n\n            name = self.dataWait(nameXpath) \\\n\n            price = self.dataWait(priceXpath)\\\n\n            if sale == 2:\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.eggFrame.loc\\[len(self.eggFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                \"False\",\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            elif sale == 3:\\\n\n                prevPriceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[3\\]'\\\n\n                prevPrice = self.dataWait(prevPriceXpath)\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.eggFrame.loc\\[len(self.eggFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                prevPrice,\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            else:\\\n\n                \\# Catch if there is anything missing elements\\\n\n                self.eggFrame.loc\\[len(self.eggFrame)\\] = \\[\"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   url\\]\\\n\n            self.count += 1\\\n\n            print(\"Eggs item added\", self.count,\" of \", total,\" :  \", name)\\\n\n        self.count = 0\n\n \n\n    def requestTomato( self ):\\\n\n        total = len(self.tomatoesUrls)\\\n\n        while self.count \\&lt; total:\\\n\n            url = self.tomatoesUrls\\[self.count\\]\\\n\n            self.driver.get(url)\\\n\n            time.sleep(1) \\# marionette Error Fix\\\n\n            pXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p'\\\n\n            nameXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/h1'\\\n\n            priceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[1\\]'\\\n\n            sale = self.dataWaitForAll(pXpath) #Ensures the page is loaded\\\n\n            name = self.dataWait(nameXpath) \\\n\n            price = self.dataWait(priceXpath)\\\n\n            if sale == 2:\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.tomatoFrame.loc\\[len(self.tomatoFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                \"False\",\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            elif sale == 3:\\\n\n                prevPriceXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[2\\]'\\\n\n                weightXpath = '//\\*\\[contains(\\@class, \"product-details_detailsContainer\")\\]/p\\[3\\]'\\\n\n                prevPrice = self.dataWait(prevPriceXpath)\\\n\n                weight = self.dataWait(weightXpath)\\\n\n                self.tomatoFrame.loc\\[len(self.tomatoFrame)\\] = \\[name,\\\n\n                                                price,\\\n\n                                                prevPrice,\\\n\n                                                weight,\\\n\n                                                url\\]\\\n\n            else:\\\n\n                \\# Catch if there is anything missing elements\\\n\n                self.tomatoFrame.loc\\[len(self.tomatoFrame)\\] = \\[\"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   \"SKIPPED\",\\\n\n                                                   url\\]\\\n\n            self.count += 1\\\n\n            print(\"Tomato item added\", self.count,\"of\", total, \": \", name)\\\n\n        self.count = 0\n\n \n\n\\# Start\\\n\nspider = HyveeSpider()\\\n\nspider.start_requests()\\\n\n#Adds the date that the data was scraped\\\n\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))\\[:-8\\]\\\n\n#To CSV files\\\n\nspider.baconFrame.to_csv(currentDate + \"Hyvee Bacon.csv\")\\\n\nspider.eggFrame.to_csv(currentDate + \"Hyvee Egg.csv\")\\\n\nspider.tomatoFrame.to_csv(currentDate + \"Hyvee Heirloom Tomatoes.csv\")\n\n\n\n\nWeek 4: (Current Week): Data Collection and Preprocessing \n\nDuring this week, the team is focused on collecting the necessary data from identified sources. The collected data will undergo preprocessing tasks such as cleaning, transforming, and formatting to ensure its suitability for analysis and modeling. \n\nWeek 5: (Next Week): Exploratory Data Analysis and Feature Engineering \n\nIn the upcoming week, the team will perform exploratory data analysis to gain insights into the collected data. Statistical techniques and visualization methods will be employed to understand the distribution, patterns, and relationships within the data. Feature engineering techniques will also be applied to extract relevant features and create new variables that enhance the predictive power of the model. \n\nWeek 6: Model Development - Building and Training the Machine Learning Model \n\nFollowing the exploratory data analysis, the team will move on to developing the machine learning model for demand forecasting. Suitable algorithms will be selected based on the problem’s nature and the available data. The chosen model(s) will be implemented and trained using the preprocessed data, with model parameters fine-tuned for optimal performance. \n\nWeek 7: Model Evaluation, Fine-tuning, and Validation \n\nDuring this phase, the trained models will be rigorously evaluated to measure their performance and predictive capabilities. Various evaluation metrics will be calculated to assess the model’s effectiveness. The team will further fine-tune the model, adjusting parameters or exploring ensemble techniques to improve its performance. Validation techniques, such as holdout sets or cross-validation, will be employed to ensure the model’s generalizability and reliability. \n\nWeek 8: Final Testing, Presentation Preparation, and Documentation \n\nIn the final week, the developed model will undergo thorough testing to ensure its stability and accuracy in predicting demand for local food products. The team will prepare a comprehensive presentation summarizing the project’s objectives, methodology, findings, and recommendations. Documentation of the project, including the data collection process, preprocessing steps, modeling techniques, and results, will also be completed. \n These future and next steps will allow the team to progress through the remaining weeks, effectively building and evaluating the machine learning model for demand forecasting in the local food industry. \nFuture scope:  \nthe future scope of the project lies in expanding the application’s capabilities, incorporating additional data sources, refining the machine learning models, and fostering collaboration within the local food ecosystem. By continually improving and adapting the solution, the project can contribute to the sustainable growth and success of Iowa’s local food producers."
  },
  {
    "objectID": "blog2023/WeekFour/Grocery-Week-Four/Grocery-Week-Four.html#project-overview",
    "href": "blog2023/WeekFour/Grocery-Week-Four/Grocery-Week-Four.html#project-overview",
    "title": "Grocery Team Week Four Wrap Up",
    "section": "Project Overview",
    "text": "Project Overview\nOur group’s task in the 2023 DSPG program is to develop a tool for the Farm, Food and Enterprise Development ISU Extension (FFED) that can help inform decision making for rural grocery stores. The goal of the project is to help users to make better decisions in opening, inheriting, and operating grocery stores in rural environments. The development of the tool relies heavily on the research conducted by domain experts on rural grocery stores and accessibility to verified data on the topic. Our current workflow moving forward is outlined in the following manner.\n\n\n\nWeekly Project Timeline\n\n\nAs we learn more about our data and possibilities for automating processes that our client’s initially calculated manually, we update our vision for what our final deliverable for DSPG could be. A draft of our thoughts are depicted as a tool which takes a certain range of user input and automated data, transforms that data through converted functions, and outputs the results in the most accessible format for the user.\n\n\n\nTool Processes"
  },
  {
    "objectID": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#project-overview",
    "href": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#project-overview",
    "title": "Housing Team Week Four Wrap Up",
    "section": "Project Overview",
    "text": "Project Overview\nThis is the project plan we came up with the first week of DSPG. This project is intended to span over three years with DPSG, and different interns will be working on it in the coming years. Thus, the project plan is ambitious for this summer."
  },
  {
    "objectID": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#problem-statement",
    "href": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#problem-statement",
    "title": "Housing Team Week Four Wrap Up",
    "section": "Problem Statement",
    "text": "Problem Statement\nThe absence of a comprehensive and unbiased assessment of housing quality in rural communities poses challenges in identifying financing gaps and effectively allocating resources for housing improvement. Consequently, this hinders the overall well-being and health of residents, impacts workforce stability, diminishes rural vitality, and undermines the economic growth of Iowa. Moreover, the subjective nature of evaluating existing housing conditions and the limited availability of resources for thorough investigations further compound the problem. To address these challenges, there is a pressing need for an AI-driven approach that can provide a more accurate and objective evaluation of housing quality, identify financing gaps, and optimize the allocation of local, state, and federal funds to maximize community benefits.\nUtilizing web scraping techniques to collect images of houses from various assessor websites, an AI model can be developed to analyze and categorize housing features into good or poor quality. This can enable targeted investment strategies. It allows for the identification of houses in need of improvement and determines the areas where financial resources should be directed. By leveraging AI technology in this manner, the project seeks to streamline the housing evaluation process, eliminate subjective biases, and facilitate informed decision-making for housing investment and development initiatives in rural communities"
  },
  {
    "objectID": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#goals-and-objectives",
    "href": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#goals-and-objectives",
    "title": "Housing Team Week Four Wrap Up",
    "section": "Goals and Objectives",
    "text": "Goals and Objectives\n\nGenerate Google Street View urls for Slater, Independence, Grundy Center, and New Hampton\nScrape available housing data for Slater, Independence, Grundy Center, and New Hampton\n\nZillow\nRealtors.com\nBeacon\nVanguard\n\nCombine data frames\nCreate AI models"
  },
  {
    "objectID": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#our-progress",
    "href": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#our-progress",
    "title": "Housing Team Week Four Wrap Up",
    "section": "Our Progress",
    "text": "Our Progress\nWe have been making good progress to complete the goals and objectives we outlined above. Since the beginning of the Data Science for the Public Good Program, we have been expanding our knowledge of data science, particularly in areas that relate to this housing project. We have been learning and covering new concepts through Data Camp. We have also watched two webinars on TidyCensus training, as well as started creating AI Models to practice with.\n\nData Camp Training:\n\nGitHub Concepts\nAI Fundamentals\nIntroduction to R\nIntermediate R\nIntroduction to the Tidyverse\nWeb Scraping in R\nIntroduction to Deep Learning with Keras\n\n\n\nTidyCensus Demographic Data Collection:\nOne of the first steps in our project was to explore the available demographic data in our selected cities and counties. We thought it valuable to understand the demographic data, and we have represented in the plots below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Test AI Models:\nThe next step was creating an AI Model. We decided to create an AI Model early in the project before finishing the housing data collection so that we had a better understanding when it came to putting everything together. The AI Model below tests for vegetation in front of houses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis Week:\n\nIn-Person Data Collection\nOn Tuesday this week, the entire DSPG program went to Slater to practice data collection in person. The housing group took this as an opportunity to collect some housing photos on the ground to use in our AI Model later on.\n\n\nGoogle Street View and URLs\nWe are getting the majority of our photos for the AI to use from Google Street View. Google has an API key that you can use to generate an image for a specific address. We spent the first half of this week pulling addresses from each of our cities and creating URLs to pull the images from Google Street View.\nWe ran into a couple of problems when doing this, the biggest of which is displayed in the images below. Because we are working with cities in rural areas, there is not Google Street View images available for every street in our cities.\n\n\n\nGoogle Street View information for Grundy Center, Iowa. For reference, population was 2,811 as of 2023.\n\n\n\n\n\nGoogle Street View information for Slater, Iowa. For reference, population was 1,639 as of 2023.\n\n\n\n\n\nGoogle Street View information for Independence, Iowa. For reference, population was 6,307 as of 2023.\n\n\n\n\n\nGoogle Street View information for New Hampton, Iowa. For reference, population was 3,368 as of 2023.\n\n\nBelow is a sample from the tables we created containing the URLs to grab the images from Google Street View.\n\n\n\n\nWeb Scraping\nOnce we were finished collecting addresses and generating URLs, we moved on to scraping the web for more images. We decided to grab images from Zillow, Realtors.com, and the county assessor pages for our cities. We were able to successfully scrape images from Zillow this week.\n\n\n\n\n\n\n\n\n\n\n\n\nHappies\n\nCompleted a succesful meeting with Erin Olson-Douglas\nFinished collecting and creating URL addresses for Google Street View images\nSince Zillow owns Trulia so we don’t have to web scrape both sites :)\nSuccessfully scraped portions of data from Zillow !\n\n\n\nCrappies\n\nWeb Scraping\nBeacon and Vanguard have anti-web scraping protections\nAngelina’s Excel doesn’t operate as expected"
  },
  {
    "objectID": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#future-plans-and-next-steps",
    "href": "blog2023/WeekFour/Housing-Week-Four/Housing-Week-Four.html#future-plans-and-next-steps",
    "title": "Housing Team Week Four Wrap Up",
    "section": "Future Plans and Next Steps",
    "text": "Future Plans and Next Steps\nOnce we are able to scrape enough images off of Zillow, Realtors.com, and the assessor pages, we will be able to move on with creating AI Models. The diagram below outlines how the AI Models will work in the next steps of out project."
  },
  {
    "objectID": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html",
    "href": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html",
    "title": "AI Local Foods Team Week Seven Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply.\n\n\n\n\n\nThis week’s tasks:\n\nConsolidated all scraped data from all different websites into one master file to be used for data analysis.\nData Analysis:\n\nImported necessary packages in Juypter Notebook for data analysis.\nStarted cleaning the data: Missing values, fixing datatypes\nStarted exploring the data using various visualizations.\n\n\n\n\nConsolidated all collected data in three master lists of Eggs, Bacon and Heirloom tomato respectively. This would let work separately on each product’s data set and would give us more insights about the dataset.\n\n\n\nStarted the data analysis by importing the necessary packages, we will keep adding more packages as we go along with analysis part.\n\n\n\n\n\nCleaning:\nWe found there were a lot of missing values in our data set for various columns\n\n\n\nWe dealt with missing values for filling NA like shown in the snapshot below:\n\n\n\n\n\n\n\n\nwe can analyze the relationship between the current price and the original price of the heirloom tomatoes.\nThis analysis can help you understand the pricing trends and calculate potential discounts or price differences.\nVisualize the distribution of current prices using histograms, box plots, or kernel density plots to gain insights into the pricing range and identify any outliers.\n\n\n\n\n\n\n\n\nAnalyze the weight of the heirloom tomatoes by comparing the weight in pounds and the true weight.\nThis analysis can help you determine if there are any variations in weight and assess the accuracy of weight measurements.\nCreating scatter plots or line plots to visualize the relationship between weight in pounds and true weight.\n\n\n\n\n\n\n\n\nAnalyze the different brands of heirloom tomatoes available in the dataset. Calculate the frequency of each brand to determine the most popular ones.\nCreating a bar chart and pie chart to visualize the distribution of brands and identify the market share of each brand.\n\n\n\n\n\n\n\n\nAnalyze the presence of organic and locally sourced heirloom tomatoes. Calculate the percentage of organic and local products in the dataset. Creating a bar chart or pie chart to visualize the proportion of organic and local tomatoes."
  },
  {
    "objectID": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "href": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "title": "AI Local Foods Team Week Seven Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply.\n\n\n\n\n\nThis week’s tasks:\n\nConsolidated all scraped data from all different websites into one master file to be used for data analysis.\nData Analysis:\n\nImported necessary packages in Juypter Notebook for data analysis.\nStarted cleaning the data: Missing values, fixing datatypes\nStarted exploring the data using various visualizations.\n\n\n\n\nConsolidated all collected data in three master lists of Eggs, Bacon and Heirloom tomato respectively. This would let work separately on each product’s data set and would give us more insights about the dataset.\n\n\n\nStarted the data analysis by importing the necessary packages, we will keep adding more packages as we go along with analysis part.\n\n\n\n\n\nCleaning:\nWe found there were a lot of missing values in our data set for various columns\n\n\n\nWe dealt with missing values for filling NA like shown in the snapshot below:\n\n\n\n\n\n\n\n\nwe can analyze the relationship between the current price and the original price of the heirloom tomatoes.\nThis analysis can help you understand the pricing trends and calculate potential discounts or price differences.\nVisualize the distribution of current prices using histograms, box plots, or kernel density plots to gain insights into the pricing range and identify any outliers.\n\n\n\n\n\n\n\n\nAnalyze the weight of the heirloom tomatoes by comparing the weight in pounds and the true weight.\nThis analysis can help you determine if there are any variations in weight and assess the accuracy of weight measurements.\nCreating scatter plots or line plots to visualize the relationship between weight in pounds and true weight.\n\n\n\n\n\n\n\n\nAnalyze the different brands of heirloom tomatoes available in the dataset. Calculate the frequency of each brand to determine the most popular ones.\nCreating a bar chart and pie chart to visualize the distribution of brands and identify the market share of each brand.\n\n\n\n\n\n\n\n\nAnalyze the presence of organic and locally sourced heirloom tomatoes. Calculate the percentage of organic and local products in the dataset. Creating a bar chart or pie chart to visualize the proportion of organic and local tomatoes."
  },
  {
    "objectID": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#several-web-scrapping-spiders-for-selected-websites-to-facilitate-the-creation-of-a-comprehensive-product-database.",
    "href": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#several-web-scrapping-spiders-for-selected-websites-to-facilitate-the-creation-of-a-comprehensive-product-database.",
    "title": "AI Local Foods Team Week Seven Wrap Up",
    "section": "Several web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database.",
    "text": "Several web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database.\nTo facilitate the creation of a comprehensive product database, we have developed several web-scraping spiders for websites such as\n1. Fresh Thyme\n2. Hy-Vee\n3. Gateway Market\n4. New Pioneer Co-op\n5. Russ’s Market\n6. Iowa Food Hub\n7. Joia Food Farm\nThese spiders automate the data scraping process, eliminating the need for repetitive data collection and significantly increasing our efficiency. It’s important to note that this list is not exhaustive, as our web-scraping spiders can be expanded to include other websites beyond those listed here."
  },
  {
    "objectID": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#showcase-the-capability-of-the-spiders-with-a-specific-crop-example.",
    "href": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#showcase-the-capability-of-the-spiders-with-a-specific-crop-example.",
    "title": "AI Local Foods Team Week Seven Wrap Up",
    "section": "Showcase the capability of the spiders with a specific crop example.",
    "text": "Showcase the capability of the spiders with a specific crop example.\nIt’s worth noting that the capability of these spiders is not limited to a specific product type; they can be utilized to extract data from a wide selection of products. As an example, we have successfully demonstrated their functionality and effectiveness in retrieving data for various products, including specific crops such as tomatoes, carrots, green onions, potatoes, spinach, lettuce, and many more.\n\n\n\n\n\nMoreover, these spiders can be further enhanced to automate the data cleaning processes which runs concurrently within the spider. This approach allows us to efficiently address data quality issues, ensuring that the collected data is reliable and ready for analysis. The main objective of this enhancement is to guarantee that the collected data is thoroughly cleaned before integrating it into a file format, providing a seamless transition for further analysis and utilization. This specific example Highlights Heirloom Tomatoes from the listed stores as mention above."
  },
  {
    "objectID": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#optimization-of-the-crop-flow-from-the-point-of-supply-to-the-point-of-demand-that-maximizes-overall-profit",
    "href": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#optimization-of-the-crop-flow-from-the-point-of-supply-to-the-point-of-demand-that-maximizes-overall-profit",
    "title": "AI Local Foods Team Week Seven Wrap Up",
    "section": "Optimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit",
    "text": "Optimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit\n\n\n\n\n\n\nThe red points represent the counties.\nThe green lines represent the flow of crops, and the blue arrow shows the direction of the flow.\nMaximizes the revenue by selling the crops.\nMinimizes the cost of distance traveled.\nRelaxed previous assumption “Supply is greater than demand”\n\nThe following might be included in the project:\n\nA separate account of fresh and not fresh products.\nConsideration of each individual farmer’s profit.\nConsideration of the flow of trucks rather than the flow of crops."
  },
  {
    "objectID": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#demand-and-supply-estimation",
    "href": "blog2023/WeekSeven/Foods-Week-Seven/Foods-Week-Seven.html#demand-and-supply-estimation",
    "title": "AI Local Foods Team Week Seven Wrap Up",
    "section": "Demand and supply estimation",
    "text": "Demand and supply estimation\n\n\n\n\n\nDisclaimer: Supply and demand estimation requires further correction, and the methodology needs improvements.\n\n\nDemand Estimation:\n\nThe population of United States is 334.9 million as of June 26, 2023. (https://www.census.gov/popclock/)\nIn 2017, fresh market consumption was 20.3 pounds per capita. (https://www.agmrc.org/commodities-products/vegetables/tomatoes)\nSo, the demand for fresh tomatoes in the United States is (334.9 million x 20.3 pounds) or 3.08 Million Metric Tons.\nThere is around 1% of the US population lives in Iowa.\nThat translates to 30.8 thousand Metric Tons of tomatoes demand in Iowa.\nConsidering the demand for tomatoes stays the same for 52 weeks. The demand for tomatoes in Iowa per week is 592 Metric Tons.\nWe will allocate this demand to each county of Iowa by population.\n\n\n\n\n\n\n\n\nSupply Estimation:\n\nThe growing season of tomatoes is between May to Mid-September in Iowa. (https://www.tomatofest.com/Tomato_Growing_Zone_Maps_s/164.htm)\nWe have 135 days between planting and harvesting tomatoes.\nTomatoes require 100 days to fully mature. However, there are some special varieties of tomatoes that require 50-60 days to mature.  (https://www.gardeningknowhow.com/edible/vegetables/tomato/planting-time-for-tomatoes.htm)\nSo, we are considering on average tomatoes take 80 days to be harvested.\nIn this context, we assumed that being mature represents the time between sowing seed and harvesting full-grown tomatoes.\nWe are only considering single cultivation of tomatoes during a year.\nSo, the tomatoes will be harvested during the timeline day 80-135 or, during late July to mid-September, that is 7.85 or 8 weeks. \nIn our calculations, we will estimate the weekly supply of tomatoes from late July to mid-September.\nIn 2020, approximately 12,619.2 tons of fresh market tomatoes were harvested from approximately 272,900 acres. (https://www.agmrc.org/commodities-products/vegetables/tomatoes)\nFlorida and California account for about two-thirds of the national fresh tomato production (Wu, F., Guan, Z., & Suh, D. H. (2017). The Effects of Tomato Suspension Agreements on Market Price Dynamics.; mentioned in https://edis.ifas.ufl.edu/publication/FE1027)\nThe remaining 48 states are responsible for one-third of the national fresh tomatoes. That is 4206.4 tons.\nTotal farming land in USA is 900.21 million acres as of 2022.\nTotal farm land in Florida is 9.73 million acres as of 2017.\nTotal farm land in California is 24.23 million acres as of 2017.\n (https://www.nass.usda.gov/AgCensus)\nRemaining 48 states have a farm land of 866.25 million acres.\nTotal farmland in Iowa is 30.56 million acres. (https://www.nass.usda.gov/AgCensus)\nConsidering tomatoes are grown uniformly in these farmlands, the proportion of farmland Iowa has compared to all 48 states (without California, Florida) is 3.52%.\nThe production of tomatoes in Iowa is (3.52% x 4206.4 tons) or, 148 tons.\nAs the production of tomatoes is distributed over 8 weeks (as mentioned in no 8), the weekly supply quantity of tomatoes is (148/8) or 18.5 tons or 16.78 Metric Tons.\nConsidering the average farm size in each county of Iowa is the same. The supply of tomatoes can be obtained from the number of farms.\n\n\n\n\n\n\nTeaser video: Video"
  },
  {
    "objectID": "blog2023/WeekSeven/Grocery-Week-Seven/Grocery_Week_Seven.html",
    "href": "blog2023/WeekSeven/Grocery-Week-Seven/Grocery_Week_Seven.html",
    "title": "Grocery Team Week Seven Wrap Up",
    "section": "",
    "text": "This week our team worked on automating the functions created previously and creating visualization from sales Genie data. We also shortlisted some ACS demographic variables to display statistics about the store location in our dashboard.\n\nWe successfully completed automating the rural population function and created the metro function population. We created the Pop_Binder function, which creates a new data frame from an address with the populations for all the cities in the county, and an Address_Parser function which parsers a given address into the street, city, state, and country. The Pop_Binder and Address_Parser will likely be used in an initialization function that will run before the rest of the toolkit and save as a global variable.\n\nWe also tried to automate the retrieval of relevant census statistics for the market area selected by a user of our tool. Careful decisions were made about the granularity and selection of the variables to use. We created a function, get_census_vars, that pulls the census data for all the variables selected.\nWe created some visualization about the distribution of the chain non, chain grocery stores, and dollar stores for cities in different classification groups. We also cleaned some data and made the last week’s graphs more consistent.\n\n\nFrom the above graph, we can see that smaller cities usually tend to have just non-chain grocery stores and not dollar stores or chain grocery stores.\n\nWe also tried to visualize cities store distribution based on Population-Store Ratio. \n\nThe above graph shows the cities in Iowa with 1 grocery store for at least every 500 people\n\n\nFinal Project Presentation Outline\nWe created our YouTube Trailer video for our project this week and also made a rough outline for the final project presentation. We are planning to divide our final presentations into 4 sections:\n\nIntroduction(Harun)\n\nProject Overview:\n\nSummary\nGoal\nThe Excel tool\n\nLiterature Review & Data Collection:\n\nReading on Rural Grocery Stores\nSelecting data to automate\n\n\n\n\nMethods\n\nMarket Size Calculation (Alex)\n\nMarket area calculation\n\nQuarter Circle \nVoronoi \nReillys \n\nPopulation\n\nRural Population\nMetro Population\nTown Population\n\n\nRevenue Estimation (Srika)\n\nAverage Grocery Spend  \nState index and percentage price index: \n\nRural price parities(RUPP) \nConsumer price index(CPI) \nUpdate and Maintenance\n\nMoney spent by different categories of shoppers \nMarket trends \nVisualization: \n\nNon-Chain grocery store, Chain grocery store, Dollar store distribution \nThe RUCC Classification \nCity group classification \n\n\nExpense Estimation (Aaron)\n\nExpenses for Opening a Store \n\nCost of Goods Sold \nOperating Expenses \nAsset Depreciation \nLoan Interest\nRent \n\nSecondary Sources of Income \n\nIncome from interest \nOther income \n\nUser-defined Inputs \n\nSwitching between multiple ownership scenarios \nInput their own percentage values as sliders \nThe assets list can be customized to calculate depreciation\nCustom loan and rent\n\nPre-Tax Profit Calculation\n\nDesign Choices (Harun)\n\n\n\nResults (Harun)\n\nDashboard Demo\n\n\n\nConclusion"
  },
  {
    "objectID": "blog2023/WeekSeven/Housing-Week-Seven/Housing_Week_Seven.html",
    "href": "blog2023/WeekSeven/Housing-Week-Seven/Housing_Week_Seven.html",
    "title": "Housing Team Week Seven Wrap Up",
    "section": "",
    "text": "For more detailed information on what each member of the housing team has accomplished thus far, check out their individual blog pages. Links are embedded in their specific sections."
  },
  {
    "objectID": "blog2023/WeekSeven/Housing-Week-Seven/Housing_Week_Seven.html#week-seven-for-the-housing-team",
    "href": "blog2023/WeekSeven/Housing-Week-Seven/Housing_Week_Seven.html#week-seven-for-the-housing-team",
    "title": "Housing Team Week Seven Wrap Up",
    "section": "",
    "text": "For more detailed information on what each member of the housing team has accomplished thus far, check out their individual blog pages. Links are embedded in their specific sections."
  },
  {
    "objectID": "blog2023/WeekSeven/Housing-Week-Seven/Housing_Week_Seven.html#final-presentation-outline",
    "href": "blog2023/WeekSeven/Housing-Week-Seven/Housing_Week_Seven.html#final-presentation-outline",
    "title": "Housing Team Week Seven Wrap Up",
    "section": "Final Presentation Outline",
    "text": "Final Presentation Outline\n\n\n\n\n\n\n\n\nSPEAKER\nSECTION\nTIME\n\n\nMorenike\n Introduction\n\nOverview of the project objectives and goals\nImportance of data collection and analysis in real estate and community analysis\n\n2 min\n\n\nAngelina and Kailyn\nData Collection\nScraping from Beacon and Vanguard\n\nBrief explanation of Beacon and Vanguard as data sources\nScraping process\nImportance of obtaining accurate and reliable data from these sources\n\nAddress Cleaning and Google Links\n\nAddress cleaning and standardization techniques\nUtilizing Google links for Google Street View image retrieval\nReasons for having accurate and complete address data\nScraping from Google Street View\nExtracting street view images for AI model use\nTechniques used for automated data extraction from Google Street View\n\nScraping from Zillow\n\nIntroduction to Zillow as a real estate data source\nExtracting relevant property data from Zillow\nChallenges and limitations of scraping from Zillow\n\n5 min\n\n\nGavin\nAngelina and Kailyn on Manual Image Sorting\nAI Model Creation\nOnly displaying images for one of the models. Note in presentation the ones that do exist, but we are using only one model to refine presentation.\nManual Image Sorting to Train Models\n\nTechniques used for organizing and categorizing the obtained images\nPurpose of image sorting\n\nBuilding AI Models (Binary and Multiple)\n\nKeras and Tensorflow Libraries\nRefining image data\nAI has to be able to easily read the images\nIdentify Labels (Binary vs. Multiple)\nSplit Training, Testing, and Evaluating Data\nBuild Layers\nTrain the AI Models\nEvaluate\nExport\n\n10 min\n\n\nGavin\nThe Thing Gavin Made that Writes to a CSV\n8 min\n\n\nKailyn\nDemographic Analysis and Profiling\nSelect Communities\n\nKey findings and insights regarding population, age groups, etc.\n\nAll Iowa Communities\n\nOverview of the demographic analysis methodology\nComparative analysis of Iowa communities based on demographic factors\nVisualization techniques used to present the analysis results\n\n6 min\n\n\nAngelina\nVisualizing Housing Quality Data from AI Model Outputs (with GIS?)\nGeocoding Addresses\n\nExplanation of geocoding process and its importance\nComparison of geocoding in R\nEvaluation of advantages and limitations of each geocoding approach\n\nMapping AI Model data\n\nApplication of spatial data for visualizing and analyzing house quality\nTechniques used for representing house quality on maps\nInterpretation and insights gained from geographical Analysis of house quality\n\n6 min\n\n\nMorenike\nConclusion\n\nSummary of the project workflow and key findings\nLessons learned and challenges encountered\nFuture possibilities and areas for further improvement\n\n2 min\n\n\n\nQuestions\n10 min"
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "",
    "text": "Lunchtime at Grundy Center."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#grocery-survey-first-impressions",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#grocery-survey-first-impressions",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Grocery Survey: First Impressions",
    "text": "Grocery Survey: First Impressions\nWe have also been looking over the results of a survey conducted by Iowa State Extension and Outreach (sponsored by AgMRC) that asked owners and senior managers of independently-owned rural grocery stores throughout Iowa a series of questions about their store’s performance, selection and design, as well as the top challenges they face in keeping their store profitable. The survey divided Iowa into 3 regions and examined the survey results accordingly.\nOne issue with the survey was the low response rate: only 95 out of 671 independent grocers responded (14.2%). However, the findings might still be useful to look at for our purposes.\nThere is much more potentially useful information, such as information about the most frequently found unique assets of stores and the specific types of local products most frequently bought according to region, shown here:\nOverall, this survey may help us provide useful information to the users of our tool and can help grant us perspective on how well the functionality of our tools maps onto the ground truth of what is really going on with rural grocery stores in Iowa. I found it to be a very interesting and useful resource.\n\nBizminer Percentages\nOne important concern for the expenses component of the project is the question of modularity, or the freedom given to the user to affect the calculations performed in the server. The functions used in this project to calculate expenses for potential grocery stores do so by providing different percentages of revenue spent on expenses based off of how high the total estimated revenue input is at the beginning. The original excel sheet establishes 5 distinct ranges that each have different percentages of the revenue across the different line items of the sheet.\n\n\n\nPercentages for categories of expenses/income across revenue ranges/store sizes (Bizminer)\n\n\nThese ranges can be thought of as stand-ins for the different sizes of grocery stores. When a store takes in a certain amount of revenue, the function assigns a budget percentage corresponding to the assumed size of the store.\nAs it stands now, many of the functions make use of nested “ifelse” statements in order to find the budget percentage of an expense or a source of income and multiply it by the total estimated revenue. For example:\n\nEmployee_Wages &lt;- function(Total_Estimated_Revenue) {\n\n  ifelse(Total_Estimated_Revenue &lt; 500000, stop(\"error: no data for this revenue range\"),\n         percentage &lt;- ifelse(Total_Estimated_Revenue &lt; 999999.99, .0789,\n                              ifelse(Total_Estimated_Revenue &lt; 2499999.99, .0934,\n                                     ifelse(Total_Estimated_Revenue &lt; 4999999.99, .0751,\n                                            ifelse(Total_Estimated_Revenue &lt; 24999999.99, .0975, .1083)))))\n\n  Total_Estimated_Revenue * percentage\n}\n\nThe function gives the user an error message if they enter a revenue amount under $500,000 dollars due to there being no data at that range.\n\n\nThe Linearity Finding\nMany of the functions derived from the excel sheet “Estimating Expenses” have been written to take the total estimated revenue as an input and return the dollar amount of a certain cost in accordance to the average percentage of revenue spent on that cost for stores of a similar size. For example, a store that takes in $1,000,000 and a store that takes in $2,000,000 share common percentages for costs such as employee wages and officer compensation. This broad assignment of one percentage based on a large revenue window also applies to the function for calculating gross margin, or the amount of funds left over after accounting for the cost of goods for the store.\nHowever, since most of these functions receive the same input of total estimated revenue and all of the percentages for the expenses are bundled together for the same window of revenue, this creates a problem. Suppose we want to compare the profitability of a store that takes in $1,000,000 of revenue vs. $2,000,000. According to the way our calculations work now, both of these stores will have the same percentage of their revenue deducted as expenses. So if 95% of our revenue goes towards covering expenses for every value that falls within the window of $1,000,000 to $2,500,000, there is a perfect linear relationship between the revenue and the profit of the store; the $1,000,000 store collects $50,000 in profit while the $2,000,000 store collects $100,000. If we were to create a numeric regular sequence of revenue inputs from $1,000,000 to $2,500,000 and plot it against one of our measures of profit (before depreciation etc.), we would get:\n\n\n\nRevenue vs. Expense\n\n\nThis is a problem because the ultimate goal of our tool is to help potential grocery store owners decide which area could be more profitable for the store that they want to open than the next area. In other words, the model doesn’t afford any insight into the relative advantage of setting up a store in a given a area versus another, because any increased revenue from a chosen area is immediately offset by a corresponding increase in expenses that affects every store within a revenue range uniformly.\n\n\nA Potential Solution\nOne way to ameliorate this problem is to simply give more control to the user of the application in choosing their own expense percentages. That way, the user can tinker with the tool in order to determine (in a broad way) how much resources they must have and how they must allocate those resources in order to be profitable in a certain location. We propose that this can be done through R Shiny. R Shiny allows for numeric inputs into functions by means of a user-controlled slider. Through this method, there is no need to discard the default percentages that we were previously working with; rather, instead of being hard-coded into the function, they can be placed as labels along the slider or set as default values that the user can adjust manually if they so choose.\n\n\n\nSlider input example (Shiny)\n\n\nThis way, the user can know the industry averages for different variables while also comparing their own plans with those averages.\n\n\nThe Rounding Problem\nAnother issue I faced was one involving the rounding of percentages of my functions. I had rounded the original percentages up or down depending on whether they were considered a cost or an expense in the balance sheet, and this affected my output significantly when I went to compare my R output to the example provided by Duane (our client) in his excel sheet. Thankfully, when I reverted the percentages to their original state (4 decimal places), my outputs then matched the one’s displayed in the excel sheet. The big takeaway from this is that most of the time, the final output is what should be rounded in a calculation and not the intermediaries. I mistakenly thought that rounding the percentages used in the calculation in certain directions could provide a more “conservative” or “risky” weight to the final prediction. However, since we were dealing with small percentages of very large numbers, the degree of rounding that was there greatly impacted overall accuracy and rendered the predictions illogical. Thankfully, the functions work as they are supposed to now (even thought they may undergo a significant rewrite later)."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#geocodingreverse-geocoding",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#geocodingreverse-geocoding",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Geocoding/Reverse Geocoding",
    "text": "Geocoding/Reverse Geocoding\nAlongside working on the expenses functions, I also looked into the problem of obtaining the names of locations within the radius of a point for the purpose of obtaining relevant population information necessary for estimating market size. Two processes are important in helping to accomplish this task: geocoding and reverse geocoding. Geocoding simply means being able to take an address or name of a location and return a set of coordinates fo that location. Reverse geocoding is its opposite: it takes a set of coordinates as input an returns an address in response.\nOne package that I found to be useful while working on this problem was revgeo. Revgeo is an R package for reverse geocoding that returns the city, state, country and zip code of a set of coordinates provided by the user.\n\nrevgeo_address &lt;- function(longitude, latitude) {\n  \n  API_KEY &lt;- Sys.getenv(\"GOOGLE_API_KEY\")\n  \n  address &lt;- revgeo::revgeo(longitude = longitude, latitude = latitude, \n                            provider = 'google', output = \"frame\",\n                            API = API_KEY)\n  \n  address\n}\n\naddress &lt;- revgeo_address(-92,41)\n\n\naddress\n\nNULL\n\n\nThis package proved to be useful for making a basic function that gets the list of nearby counties within a radius of a set of coordinates. However, the biggest flaw is that the “county” column isn’t accurate; it lists the name of the country instead. For our purposes, this was a significant roadblock, as having the county of a point makes retrieving data from TidyCensus a lot easier.\nOne issue we’ve confronted with collecting data from geocoding is how to deal with coordinates near a state boundary. Because of the way that the TidyCensus library collects geography information, getting data for areas outside of state lines is not a straightforward process. Therefore, we have been brainstorming ways to get this interstate data without placing too high of a demand on memory and performance."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#google-maps-api",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#google-maps-api",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Google Maps API",
    "text": "Google Maps API\nWe used the Google Maps API to find the nearest grocery stores. An API is a tool developers provide for other developers that allows them to incorporate their work into other works. The API gives us the latitude and longitude of the other stores, which we can use to find the distance between the stores. We can also use these coordinates to generate polygons."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#quarter-circle",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#quarter-circle",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Quarter Circle",
    "text": "Quarter Circle\nThe quarter circle was the original design in the excel spreadsheet. Quarter circles are very simple to implement, and work by finding the radius to the nearest store and turning it into a quarter circle that we are using as the region of our market share. The disadvantage of doing it this way is that it is a massive over estimate because it doesn’t factor the other store’s “sphere of influence” into account. These are essentially a rudimentary version of a Voronoi polygon."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#automating-population-data",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#automating-population-data",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Automating Population Data",
    "text": "Automating Population Data\nWe are able to use census information to get data at various different resolutions, such as state, county, tract, and place. Using this we can generate maps and map regions to do different types of analysis. One example is knowing how many people live in a region between two grocery stores.\n\nCounty: We have census data for the populations of counties, which we can retrieve using TidyCensus. TidyCensus is a package for R which allows developers to easily get Census data.\nTown: Using the script Jay made, we can find the populations of all the towns in a given geometry. Jay’s script uses a package called Tigris, which allows us to see demographic data within a geographic boundary. Jay’s script has functions that allow you to give a latitude, longitude, and state, and it will return the city name. You can also give it a county and have it return all the towns in that county.\nRural: We are in the process of automating the rural population. We can calculate this by adding up the populations of all the towns in the county and subtracting that from the total number of people in the county."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#voronoi",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#voronoi",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Voronoi",
    "text": "Voronoi\nVoronoi polygons are a way of breaking up a plane into sections defined by which node of a graph they are closest to. This shows a model where people will go to the closest grocery store purely based on their location. This doesn’t take into account anything about the store, and only cares about the location. The pro’s of this are that in rural settings, people are most likely going to go to the nearest store. The cons of this are that it ignores factors such as offerings, pricing, and the fact that people tend to go towards more densely populated areas to shop.\n\nVoronoi Polygon demonstration from Wikipedia\n\nVoronoi diagram of grocery stores in Iowa"
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#reillys-law",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#reillys-law",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Reilly’s Law",
    "text": "Reilly’s Law\nReilly’s law of retail gravitation is the idea that people are drawn to shop in areas with denser populations. The formula for Reilly’s law is a ratio of distance between cities and the difference in the populations between the two. This is a good model because it takes into account the shopping habits of consumers. The downsides of this model are that it has a lot of limitations, such as the cities having to have relatively similar populations, and it gives an overestimate because it assumes everyone shops locally."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#voronoi-vs-reillys-law",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#voronoi-vs-reillys-law",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Voronoi vs Reilly’s Law",
    "text": "Voronoi vs Reilly’s Law\nVoronoi and Reilly’s law can tell us similar, but different things. Voronoi shows the geographic regions where our proposed store would be the closest. Reilly’s law would show the regions around our store that people would be willing to travel in order to shop at the store."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#difficulty-in-the-process",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#difficulty-in-the-process",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Difficulty in the process",
    "text": "Difficulty in the process\nFiguring out the State index and percentage price increase values took a lot of my time after I created the functions. I shortlisted some of the resources that I found can be useful, like the CPI(Consumer Price Index), PPI(Producer Price Index), RPP(Rural Price Parities), and Cost of Living index. What made it more confusing for me was that some of these had values specifically for groceries other than the overall value. After discussing with Duane(our client) in our last meeting, I chose to use the Consumer Price index, which is the measure of the average change over time in the prices paid by urban consumers for a market basket of consumer goods and services, as the estimated price increase, and use Rural Price Parities, which measure the differences in price levels across states and metropolitan areas for a given year and are expressed as a percentage of the overall national price level, as the State Index."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#update-and-maintenance",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#update-and-maintenance",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Update and Maintenance",
    "text": "Update and Maintenance\nI created the following table to Track the source and links about the variables used and how frequently it is required to be updated.\n\n\n\n\n\n\n\n\n\n\nVariable name\nFrequency\nSource\nLink\nNotes\n\n\nTotal US Grocery Sales\nOptional\nIBIS\n\nDefault base year taken as 2022\n\n\nTotal US population\nOptional\nUS Census Bureau\n\nDefault base year taken as 2022\n\n\nEstimated cumulative price increase(CPI)\nyearly update/ Half yearly update\nUS Bureau of labor statistics\nhttps://data.bls.gov/timeseries/CUUR0000SA0\nCPI in current year - CPI in base year\nFor now defaulting as 7 for 2023\n\n\nState Index\nyearly update\nBEA\nhttps://tinyurl.com/ycwpjzwz\nDepends on state\n\n\n\nThe consumer Price index is updated every month, but it is fine to take yearly once or twice because it does not change each month very much. And for the Rural Price parities, the latest data that we have is 2021 because it is economic census data and is not available for the recent 1 year."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#sales-genie-data-set-vs-google-api-data-set",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#sales-genie-data-set-vs-google-api-data-set",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "Sales Genie Data Set Vs Google API data set",
    "text": "Sales Genie Data Set Vs Google API data set\nI was working with the Sales Genie Data sets about the grocery stores and dollar stores in Iowa. I also looked at the Google API data set for the dollar store data in Iowa and Illinos. Most of the visualizations that I created were using the sales Genie Data Sets. Although one problem that I noticed was that it could possibly be missing some stores. I noticed this when I was trying to plot the number of stores in each Dollar Store chain using both the Google API data set and the Sales Genie Data set.\n\nThe above plot shows the number of cities in Iowa with more than 1 dollar store plotted using the Sales Genie Data Set. We notice that Ames is not there, although there are 2 dollar stores in Ames.\nWhen I saw the Google API data set to check this, it was also incorrect, saying there are 4 dollar stores in Ames. When I looked at the data table, I found that it had a store which was near Ames, which had the mailing city as Ames. I have more difficulty with the Google data set is that it is not cleaned it has some dollar generals as DG market and differentiated based on name than on chain like shown in the below plot:\n\nThe following plots show the distribution of stores in cities of Iowa with more than one chain grocery store and non-chain grocery store, respectively. I am currently working on improving this plot by trying to make a single plot that contains the distribution of the stores color-coded by the type so that we can see if there are any trends that we can observe about the presence of one type of store affecting the other(like if dollar stores affect the non-chain grocery stores).\n\n\nFrom all these plots, one common thing that we can observe is that irrespective of the type, there are usually more stores in big cities than in small cities, as we would expect.\n\nSales Volume Distribution\n\nlibrary(ggplot2)\nsales_chain_grocery &lt;- ggplot(data = chain_grocery_store_data,\n                              aes(x = Location_Sales_Volume_Range,\n                                  fill = Company_Name)) +\n            geom_bar(position = \"stack\") +\n  ggtitle(\"Location Sales Volume distribution for Chain Grocery Stores\") +\n  xlab(\"Sales Volume Range\") + \n  ylab(\"Number of Stores\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\nggplotly(sales_chain_grocery)\n\n\nThe above plot shows the distribution of the number of stores by Sales volumes for the major chain grocery stores in Iowa. I must do data cleaning and arrange the x-axis titles in ascending order to make them more interpretable.\n\nThe plot shows the location sales volume for the non-chain grocery stores. We can see that many of the non-chain grocery stores have a 1-2.5 Million sales volume."
  },
  {
    "objectID": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#rucc",
    "href": "blog2023/WeekSix/Grocery-Week-Six/Grocery_Week_Six.html#rucc",
    "title": "Grocery Team Week Six Wrap Up",
    "section": "RUCC",
    "text": "RUCC\nThe 2013 Rural-Urban Continuum Codes form a classification scheme that distinguishes metropolitan counties by the population size of their metro area and non metropolitan counties by the degree of urbanization and adjacency to a metro area.\nEach county in the U.S. is assigned one of the nine codes listed below. Codes 4-9 are typically considered to be rural.\n\nRUCC 1: Counties in metro areas of 1 million population or more\nRUCC 2: Counties in metro areas of 250,000 to 1 million population\nRUCC 3: Counties in metro areas of fewer than 250,000 population\nRUCC 4: Population of 20,000 or more, adjacent to a metro area\nRUCC 5: Population of 20,000 or more, not adjacent to a metro area\nRUCC 6: Population of 2,500 to 19,999, adjacent to a metro area\nRUCC 7: Population of 2,500 to 19,999, not adjacent to a metro area\nRUCC 8: Less than 2,500 population, adjacent to a metro area\nRUCC 9: Less than 2,500 urban population, not adjacent to a metro area\n\nMatching the city names in the Sales Genie Data Set to their counties will help us make a plot classifying the stores based on their location according to the RUCC (Rural-Urban Continuum Codes). This could help us visualize if there is any correlation between the number of a particular type of store and urban or rural counties"
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "New Hampton records in the Fulcrum app.\n\n\nOn Monday and Tuesday of week six, the DSPG group traveled to Grundy Center, New Hampton and Independence, Iowa. In groups of 3-4 people, we walked around residential areas and observed the condition of houses and lots. The Fulcrum app was used to record observations. We assigned good, fair and poor ratings to characteristics such as: Roof, gutter, landscape, siding, and sidewalk to the house.\n\n\n\n\n\nWhile in residential neighborhoods, we also gave general impressions of blocks by observing presence and quality of: neighborhood sidewalks, street lights, way-finding signs, storm drainage systems, curb cuts and street trees.\n\n\n\nOne of the goals of the project has been to provide demographic profiles of three cities: Independence, New Hampton and Grundy Center. We want to show characteristics related to economics, housing and population, and how these characteristics have been changing through the years or how they are expected to change in the future. Several graphs were made in R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo far, all models needed have been implemented! The accuracy will continue to improve the more they are trained. Now what is needed is a way to get the model outputs into a format that is useful for us. We need to write the outputs to a csv file. The script below is used to write to a CSV file in the correct attribute column based on the address being evaluated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of our clients wishes to see something related to the project in the form of maps. One idea is to use maps to display well-trained AI model ratings for landscapes, siding, gutters etc. This will allow us and our clients to visualize where houses in good condition are versus lower condition.\nTo visualize addresses for the WinVEST cities and Slater, first we needed to geocode them (get a latitude and longitude for each address). Base code was from storybench.\n\n\n\n\n\n\n\n\n\n\nAddresses are plotted in QGIS using the coordinates. There are some addresses that are not in the correct place (highlighted below). This is something to look into next week.\n\n\n\n\n\n\n\nA sample map was created in which gutter quality for houses in the city are shown.\n\n\n\nLegend is still in progress. The darkest orange is “poor”, lighter orange is “fair”, and the blue is “good”.\n\n\n\n\n\n\nOn Thursday we met online with Tara Brueggeman, an Assessor for Mason City, and Erin Mullenix, one of our clients. We learned many things from this meeting. These are our main takeaways:\n\nThe data provided by Beacon and Vanguard is public but we are not able to web scrape. However, city and county assessor offices can get some data from Vanguard:\n\nA city/county assessor office has access to the Vanguard assessment data management system. Technically, they can run some customized reports on the data using SQL but many offices, especially in smaller IA communities, don’t know how to get data from the system using SQL.\n\nWe learned about the appraisal process Tara and her team follow:\n\nAppraisal starts with the use of a blueprint. Find information on number of bedrooms and bathrooms, square footage, and details on basement.\nAppraiser goes to the house to measure and inspect in-person. They used to go inside houses more often, but now homeowners are less comfortable with that. It does help to take a look at the inside of houses, especially because the inside does not always match blueprint. However, the condition of the exterior is usually a good indicator of the condition of the interior.\n\nTara will provide us with Iowa Real Property appraisal manual, which is is given to assessors. This may be useful for us because we could get better ideas of how different characteristics of a house are assessed.\nHow they aim to minimize bias when assessing?\n\nNo assessing when hungover, having a bad day, or if it’s raining outside.\nThe records are reviewed by another person before being finalized.\n\n\n\n\n\nAs we get down to the last few weeks, we are beginning to finish up major project tasks.\n\n\n\nIt’s taking a while to figure out how to write the model’s ratings to a csv file both effectively and efficiently.\n\n\n\n\nMeet with Erin Olsen-Douglas and Erin Mullenix (our clients) to give an update on what we have accomplished since last meeting. We plan to come prepared with several questions.\nCreate a teaser video and slides for our project.\nFinish script that will allow us to have the model’s outputs in a csv file.\nUse files to visualize the quality of houses.\nFurther train the models using new images of houses in Des Moines.\n\n\n\n\nWhat was the weirdest or most shocking conversation you had with a resident while collecting data?"
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#winvest-project",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#winvest-project",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "New Hampton records in the Fulcrum app.\n\n\nOn Monday and Tuesday of week six, the DSPG group traveled to Grundy Center, New Hampton and Independence, Iowa. In groups of 3-4 people, we walked around residential areas and observed the condition of houses and lots. The Fulcrum app was used to record observations. We assigned good, fair and poor ratings to characteristics such as: Roof, gutter, landscape, siding, and sidewalk to the house.\n\n\n\n\n\nWhile in residential neighborhoods, we also gave general impressions of blocks by observing presence and quality of: neighborhood sidewalks, street lights, way-finding signs, storm drainage systems, curb cuts and street trees."
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#demographic-profiles",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#demographic-profiles",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "One of the goals of the project has been to provide demographic profiles of three cities: Independence, New Hampton and Grundy Center. We want to show characteristics related to economics, housing and population, and how these characteristics have been changing through the years or how they are expected to change in the future. Several graphs were made in R."
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#progress-on-ai-models",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#progress-on-ai-models",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "So far, all models needed have been implemented! The accuracy will continue to improve the more they are trained. Now what is needed is a way to get the model outputs into a format that is useful for us. We need to write the outputs to a csv file. The script below is used to write to a CSV file in the correct attribute column based on the address being evaluated."
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#mapping",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#mapping",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "One of our clients wishes to see something related to the project in the form of maps. One idea is to use maps to display well-trained AI model ratings for landscapes, siding, gutters etc. This will allow us and our clients to visualize where houses in good condition are versus lower condition.\nTo visualize addresses for the WinVEST cities and Slater, first we needed to geocode them (get a latitude and longitude for each address). Base code was from storybench.\n\n\n\n\n\n\n\n\n\n\nAddresses are plotted in QGIS using the coordinates. There are some addresses that are not in the correct place (highlighted below). This is something to look into next week.\n\n\n\n\n\n\n\nA sample map was created in which gutter quality for houses in the city are shown.\n\n\n\nLegend is still in progress. The darkest orange is “poor”, lighter orange is “fair”, and the blue is “good”."
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#meeting-with-mason-city-assessor",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#meeting-with-mason-city-assessor",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "On Thursday we met online with Tara Brueggeman, an Assessor for Mason City, and Erin Mullenix, one of our clients. We learned many things from this meeting. These are our main takeaways:\n\nThe data provided by Beacon and Vanguard is public but we are not able to web scrape. However, city and county assessor offices can get some data from Vanguard:\n\nA city/county assessor office has access to the Vanguard assessment data management system. Technically, they can run some customized reports on the data using SQL but many offices, especially in smaller IA communities, don’t know how to get data from the system using SQL.\n\nWe learned about the appraisal process Tara and her team follow:\n\nAppraisal starts with the use of a blueprint. Find information on number of bedrooms and bathrooms, square footage, and details on basement.\nAppraiser goes to the house to measure and inspect in-person. They used to go inside houses more often, but now homeowners are less comfortable with that. It does help to take a look at the inside of houses, especially because the inside does not always match blueprint. However, the condition of the exterior is usually a good indicator of the condition of the interior.\n\nTara will provide us with Iowa Real Property appraisal manual, which is is given to assessors. This may be useful for us because we could get better ideas of how different characteristics of a house are assessed.\nHow they aim to minimize bias when assessing?\n\nNo assessing when hungover, having a bad day, or if it’s raining outside.\nThe records are reviewed by another person before being finalized."
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#things-that-went-well",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#things-that-went-well",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "As we get down to the last few weeks, we are beginning to finish up major project tasks."
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#things-that-didnt-go-so-well",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#things-that-didnt-go-so-well",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "It’s taking a while to figure out how to write the model’s ratings to a csv file both effectively and efficiently."
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#some-plans-for-next-week",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#some-plans-for-next-week",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "Meet with Erin Olsen-Douglas and Erin Mullenix (our clients) to give an update on what we have accomplished since last meeting. We plan to come prepared with several questions.\nCreate a teaser video and slides for our project.\nFinish script that will allow us to have the model’s outputs in a csv file.\nUse files to visualize the quality of houses.\nFurther train the models using new images of houses in Des Moines."
  },
  {
    "objectID": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#questions",
    "href": "blog2023/WeekSix/Housing-Week-Six/Housing_Week_Six.html#questions",
    "title": "Housing Team Week Six Wrap Up",
    "section": "",
    "text": "What was the weirdest or most shocking conversation you had with a resident while collecting data?"
  },
  {
    "objectID": "blog2023/WeekThree/Foods-Week-Three/Foods_Week_Three.html",
    "href": "blog2023/WeekThree/Foods-Week-Three/Foods_Week_Three.html",
    "title": "AI Local Foods Team Week Three Wrap Up",
    "section": "",
    "text": "The currents project objectives for this week was to\n\nCatch up on any additional training.\nCollect and find data on heirloom tomatoes, eggs, and bacon.\nLearning how to do web scraping in python through Datacamp.\nBuilding programs to do web scraping"
  },
  {
    "objectID": "blog2023/WeekThree/Foods-Week-Three/Foods_Week_Three.html#current-project-objectives",
    "href": "blog2023/WeekThree/Foods-Week-Three/Foods_Week_Three.html#current-project-objectives",
    "title": "AI Local Foods Team Week Three Wrap Up",
    "section": "",
    "text": "The currents project objectives for this week was to\n\nCatch up on any additional training.\nCollect and find data on heirloom tomatoes, eggs, and bacon.\nLearning how to do web scraping in python through Datacamp.\nBuilding programs to do web scraping"
  },
  {
    "objectID": "blog2023/WeekThree/Foods-Week-Three/Foods_Week_Three.html#works-in-progress",
    "href": "blog2023/WeekThree/Foods-Week-Three/Foods_Week_Three.html#works-in-progress",
    "title": "AI Local Foods Team Week Three Wrap Up",
    "section": "Works in Progress",
    "text": "Works in Progress\nAn excel sheet that contains a list of small businesses of Iowa grocers that we had to go through and find places that had the data we wanted.\n\n\nThese are some examples of what we were looking for\n\n\nAlong with some manual data scraping. We started work on some data scraping programs (spiders). This code block is a spider that I recently created. However, there are still some improvements that still need to be made.\n\nimport pandas as pd\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging\n\nclass FreshThymeBaconSpider(scrapy.Spider):\n    name = 'Fresh Thyme Market Bacon Spider'\n\n    def start_requests( self ):\n        start_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage']\n        for url in start_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse)\n    \n    def cardsParse(self, response):\n        #Fail safe for links\n        try:\n            #grabs all cards from list and saves the link to follow\n            xpath = '//*[contains(@class,\"Listing\")]/div/a/@href'\n            listCards = response.xpath(xpath)\n            linklist.append(listCards.extract())\n            for url in listCards:\n                yield response.follow( url = url, callback = self.itemParse, meta={'link': url} )\n        except AttributeError:\n           pass\n    \n    def itemParse(self, response):\n        #xpaths to the name and price\n        nameXpath = '//*[contains(@class, \"PdpInfoTitle\")]/text()'\n        priceXpath = '//*[contains(@class, \"PdpMainPrice\")]/text()'\n        url = response.meta.get('link')\n        #Grabs the name and price from the xpaths and adds them to the bacon list\n        bacon.append({'bacon': response.xpath(nameXpath).extract(), 'price': response.xpath(priceXpath).extract()})\n\n# Start\nconfigure_logging()\nbacon = []\nlinklist = []\nprocess = CrawlerProcess()\nprocess.crawl(FreshThymeBaconSpider)\nprocess.start()\nprocess.stop()\nbaconFrame = pd.DataFrame(bacon)\nprint(baconFrame)\n\nThis image shows an example output of data we were able to scrape."
  },
  {
    "objectID": "blog2023/WeekThree/Foods-Week-Three/Foods_Week_Three.html#dspg-questions",
    "href": "blog2023/WeekThree/Foods-Week-Three/Foods_Week_Three.html#dspg-questions",
    "title": "AI Local Foods Team Week Three Wrap Up",
    "section": "DSPG Questions",
    "text": "DSPG Questions\n\nAre there stores or market places that would be helpful for us to look into?\nIs anyone experienced in web scraping and if so there any advice that you have for us?"
  },
  {
    "objectID": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html",
    "href": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html",
    "title": "Grocery Team Week Three Wrap Up",
    "section": "",
    "text": "This week was primarily centered on clarifying the focus of our project, as well as on the retrieval of relevant sources of data. During the first part of the week, our group searched TidyCensus, the USDA and various other internet sources for data that could potentially be useful for whichever type of tool that we and our client decide on.\nAfter this, we met with our client and some of his associates at the the research park to discuss the direction of the project. We had them answer various questions on Mentimeter and showed them the list of datasets under consideration. They also gave us numerous suggestions about sources they were familiar with and told us that they would help grant us access to those of which were not immediately available to us.\nHere are some of the Mentimeter results from our meeting:\n\n\n\nAs we are wrapping up this week, we have now shifted our focus to compiling our selected data to our repository and filtering through sources that may not be as useful as others. We are continuing to think of ways to optimize this process, as well as remaining open to other sources of data or information that we have yet to discover. Here are some of the resources under consideration:"
  },
  {
    "objectID": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#current-project-objectives",
    "href": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#current-project-objectives",
    "title": "Grocery Team Week Three Wrap Up",
    "section": "",
    "text": "This week was primarily centered on clarifying the focus of our project, as well as on the retrieval of relevant sources of data. During the first part of the week, our group searched TidyCensus, the USDA and various other internet sources for data that could potentially be useful for whichever type of tool that we and our client decide on.\nAfter this, we met with our client and some of his associates at the the research park to discuss the direction of the project. We had them answer various questions on Mentimeter and showed them the list of datasets under consideration. They also gave us numerous suggestions about sources they were familiar with and told us that they would help grant us access to those of which were not immediately available to us.\nHere are some of the Mentimeter results from our meeting:\n\n\n\nAs we are wrapping up this week, we have now shifted our focus to compiling our selected data to our repository and filtering through sources that may not be as useful as others. We are continuing to think of ways to optimize this process, as well as remaining open to other sources of data or information that we have yet to discover. Here are some of the resources under consideration:"
  },
  {
    "objectID": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#works-in-progress",
    "href": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#works-in-progress",
    "title": "Grocery Team Week Three Wrap Up",
    "section": "Works in Progress",
    "text": "Works in Progress"
  },
  {
    "objectID": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#aaron",
    "href": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#aaron",
    "title": "Grocery Team Week Three Wrap Up",
    "section": "Aaron",
    "text": "Aaron\nThis week has been less coding-centric than the previous week, as many of our efforts have been centered on uncovering useful sources of data. However, I have tried to work through ways to accelerate the time-consuming process of labeling and pushing CSVs of tables of ACS data.\nHere is a function that I’ve been using to speed the process up. It’s a data labeler function that takes “table”, “year” and “geography as arguments and outputs a labelled ACS5 data table by joining labels from the”load_variables()” function in TidyCensus by the common variable code on both dataframes.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidycensus)\nlibrary(stringr)\nlibrary(readr)\n\n\n\ndata_labeler_Iowa &lt;- function(table, year, geography){\n  df &lt;- tidycensus::get_acs(geography = geography,\n                table = table,\n                state = \"IA\",\n                year = year,\n                survey = \"acs5\")\n  \n  \n  vars &lt;- tidycensus::load_variables(year, \"acs5\")\n  \n  vars &lt;- vars %&gt;%\n    dplyr::filter(stringr::str_detect(name, table))\n  \n  vars &lt;- vars %&gt;% \n    dplyr::rename(variable = name)\n  \n  vars &lt;- vars %&gt;%\n    dplyr::select(variable, label)\n  \n  df2 &lt;- merge(df,vars)\n  \n  df2 %&gt;%\n    dplyr::arrange(GEOID)\n}\n\nMedian_Household_Income_County_2021 &lt;- data_labeler_Iowa(\"B19013\", 2021, \"county\")\n\nGetting data from the 2017-2021 5-year ACS\n\n\nLoading ACS5 variables for 2021 from table B19013. To cache this dataset for faster access to ACS tables in the future, run this function with `cache_table = TRUE`. You only need to do this once per ACS dataset.\n\n\n\nhead(Median_Household_Income_County_2021)\n\n    variable GEOID                   NAME estimate  moe\n1 B19013_001 19001     Adair County, Iowa    57944 4047\n2 B19013_001 19003     Adams County, Iowa    57981 7853\n3 B19013_001 19005 Allamakee County, Iowa    59461 2644\n4 B19013_001 19007 Appanoose County, Iowa    46900 5645\n5 B19013_001 19009   Audubon County, Iowa    54643 5861\n6 B19013_001 19011    Benton County, Iowa    72334 3626\n                                                                                         label\n1 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n2 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n3 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n4 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n5 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n6 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n\n\nBuilding on this, I tried to iterate through all of these arguments by using nested for loops and writing csvs automatically. However, for a reason I don’t entirely understand, I haven’t gotten it to work the way I should. It’s possible that this has to do with the limitations of the API call and its speed within the loop.\n\nyearlist &lt;- c(2009, 2012, 2016, 2021)\ngeo_list &lt;- c(\"county\", \"tract\")\nIncome_table_list &lt;- c(\"B19013\", \"B19113\", \"B19202\", \"B19051\", \"B19055\")\nacs_table &lt;- NULL\n\n\nfor(table in Income_table_list)\n\n  for(year in yearlist)\n  \n    for(geography in geo_list)\n    \n      acs_table &lt;- tidycensus::get_acs(geography = geography,\n                                       table = table,\n                                       state = \"IA\",\n                                       year = year,\n                                       survey = \"acs5\")\n      \n      write_csv(acs_table, file = sprintf(\"%s_%s_%s\", table, year, geography))\n\nHowever, even if this didn’t completely work, I learned a lot of useful information, such as the “sprintf” function that provides placeholders for strings that are named from iterating variables in loop, as well as how to call only necessary elements of a library into a function to prevent using too much memory/slowing run time."
  },
  {
    "objectID": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#alex",
    "href": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#alex",
    "title": "Grocery Team Week Three Wrap Up",
    "section": "Alex",
    "text": "Alex\n\nData Exploration\nI spent a lot of time this week doing data exploration. I found the USDA food atlas data set, as well as the Economic Census data. I also explored the TinyUSDA package.\n\n\nClient Meeting\nThis week I met with our clients, where we clarified the scope of the project, discussed the goals moving forwards, and shared our progress. I shared about the USDA food atlas and Economic Census data sets\n\n\nSQL learning\nThis week I spent time learning PostgreSQL on DataCamp. I also did several other courses related to data visualizations and fundamental statistical skills."
  },
  {
    "objectID": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#srika",
    "href": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#srika",
    "title": "Grocery Team Week Three Wrap Up",
    "section": "Srika",
    "text": "Srika\n\nTraining:\n\nData Camp courses completed:\nIntroduction to statistics in R\nData visualization with R (in progress)\n\n\n\nCreated summaries of the client reports:\nUnderstanding the market trends and margins for different departments in a rural grocery store.\n\n\nCollected some ACS Data\nBrowsed for other possible useful sources to consider"
  },
  {
    "objectID": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#dspg-questions",
    "href": "blog2023/WeekThree/Grocery-Week-Three/Grocery-Week-Three.html#dspg-questions",
    "title": "Grocery Team Week Three Wrap Up",
    "section": "DSPG Questions",
    "text": "DSPG Questions\n\nWhat is the best way to scrape text off of a webpage with R?\nWhat are some general rules/best practices for writing efficient and reliable functions?\nWhat is differential privacy?"
  },
  {
    "objectID": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html",
    "href": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html",
    "title": "Housing Team Week Three Wrap Up",
    "section": "",
    "text": "This week, our team was able to spend time thinking and talking about:\n\nOur takeaways from last week’s client meeting\nProject goals\nMethods to use when working to reach project goals\nNew ideas for our project\n\n\n\n\nA rough plan for our project.\n\n\n\n\n\nWeb Scraping in R\nIntermediate R\n\n\n\n\nWe are currently trying to scrape data from housing assessor websites such as Iowa Assessors (data from Beacon and Vanguard) and Trulia. If we are able to successfully do so, images of houses and other information can be utilized to train our AI model(s).\nKailyn, Gavin and I (Angelina) focused on learning more about web scraping using R through the DataCamp Course called Web Scraping in R. We also began to follow steps for scraping the web using other tutorials.\n\n\n\nTo get an idea of how to use AI for the project, a sample model was trained to determine if a house has vegetation or no vegetation. About 750 images in total were used to train, validate and test the model.\nThe model was successfully trained and process did not take long.\n\n\n\n\n\n\n\n\n\n\n\nCreation of AI model was quicker than anticipated. We can spend more time into other aspects of the project (ex. data collection).\nKailyn was able to set up her blog!\nWe now have a better idea of the project’s methods and goals.\n\n\n\n\n\nStarting the web scraping process. When it comes to web scraping, we need to understand more than what the DataCamp course was teaching.\n\n\n\n\n\nGet a rough database running with housing info. We may be able to utilize a web interface that will allow us to more efficiently filter through houses.\nFamiliarize ourselves with web scraping.\nMeet with client(s) to show the project’s progress.\nFinish scraping data for Independence, Slater, New Hampton, Grundy Center, or other cities if needed.\n\n\n\n\nHow many of you have some web scraping experience?"
  },
  {
    "objectID": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#project-progress",
    "href": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#project-progress",
    "title": "Housing Team Week Three Wrap Up",
    "section": "",
    "text": "This week, our team was able to spend time thinking and talking about:\n\nOur takeaways from last week’s client meeting\nProject goals\nMethods to use when working to reach project goals\nNew ideas for our project\n\n\n\n\nA rough plan for our project.\n\n\n\n\n\nWeb Scraping in R\nIntermediate R\n\n\n\n\nWe are currently trying to scrape data from housing assessor websites such as Iowa Assessors (data from Beacon and Vanguard) and Trulia. If we are able to successfully do so, images of houses and other information can be utilized to train our AI model(s).\nKailyn, Gavin and I (Angelina) focused on learning more about web scraping using R through the DataCamp Course called Web Scraping in R. We also began to follow steps for scraping the web using other tutorials.\n\n\n\nTo get an idea of how to use AI for the project, a sample model was trained to determine if a house has vegetation or no vegetation. About 750 images in total were used to train, validate and test the model.\nThe model was successfully trained and process did not take long."
  },
  {
    "objectID": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#happies",
    "href": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#happies",
    "title": "Housing Team Week Three Wrap Up",
    "section": "",
    "text": "Creation of AI model was quicker than anticipated. We can spend more time into other aspects of the project (ex. data collection).\nKailyn was able to set up her blog!\nWe now have a better idea of the project’s methods and goals."
  },
  {
    "objectID": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#crappies",
    "href": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#crappies",
    "title": "Housing Team Week Three Wrap Up",
    "section": "",
    "text": "Starting the web scraping process. When it comes to web scraping, we need to understand more than what the DataCamp course was teaching."
  },
  {
    "objectID": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#some-plans-for-next-week",
    "href": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#some-plans-for-next-week",
    "title": "Housing Team Week Three Wrap Up",
    "section": "",
    "text": "Get a rough database running with housing info. We may be able to utilize a web interface that will allow us to more efficiently filter through houses.\nFamiliarize ourselves with web scraping.\nMeet with client(s) to show the project’s progress.\nFinish scraping data for Independence, Slater, New Hampton, Grundy Center, or other cities if needed."
  },
  {
    "objectID": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#questions",
    "href": "blog2023/WeekThree/Housing-Week-Three/Housing-Week-Three.html#questions",
    "title": "Housing Team Week Three Wrap Up",
    "section": "",
    "text": "How many of you have some web scraping experience?"
  },
  {
    "objectID": "blog2023/WeekTwo/Foods-Week-Two/Foods-Week-Two.html",
    "href": "blog2023/WeekTwo/Foods-Week-Two/Foods-Week-Two.html",
    "title": "AI Local Foods Team Week Two Wrap Up",
    "section": "",
    "text": "Completed Team exercise on first day. Analyzed the areas where each team member was proficient in and the areas where we need to work on.\nTeam members started working on datacamp. Special shout out to Aaron for being the datacamp champ of our group!\nAttended meeting with Courtney Long to discuss project expectations and also clarified questions regarding the project\n\n\n\n\nNone in progress for Week 1 except datacamp. Outputs of first exercise attached below.\n\n\n\n\n\n\nTreat this section as a place to put all of the different questions that team members have about the work they’ve been completing so far. This can be things like;\n\nHow to best create a certain plot.\nWhat package/module might be the best fit for the task.\nAn idea that can be discussed with the larger group for additional thoughts."
  },
  {
    "objectID": "blog2023/WeekTwo/Foods-Week-Two/Foods-Week-Two.html#completed-objectives",
    "href": "blog2023/WeekTwo/Foods-Week-Two/Foods-Week-Two.html#completed-objectives",
    "title": "AI Local Foods Team Week Two Wrap Up",
    "section": "",
    "text": "Completed Team exercise on first day. Analyzed the areas where each team member was proficient in and the areas where we need to work on.\nTeam members started working on datacamp. Special shout out to Aaron for being the datacamp champ of our group!\nAttended meeting with Courtney Long to discuss project expectations and also clarified questions regarding the project"
  },
  {
    "objectID": "blog2023/WeekTwo/Foods-Week-Two/Foods-Week-Two.html#works-in-progress",
    "href": "blog2023/WeekTwo/Foods-Week-Two/Foods-Week-Two.html#works-in-progress",
    "title": "AI Local Foods Team Week Two Wrap Up",
    "section": "",
    "text": "None in progress for Week 1 except datacamp. Outputs of first exercise attached below."
  },
  {
    "objectID": "blog2023/WeekTwo/Foods-Week-Two/Foods-Week-Two.html#dspg-questions",
    "href": "blog2023/WeekTwo/Foods-Week-Two/Foods-Week-Two.html#dspg-questions",
    "title": "AI Local Foods Team Week Two Wrap Up",
    "section": "",
    "text": "Treat this section as a place to put all of the different questions that team members have about the work they’ve been completing so far. This can be things like;\n\nHow to best create a certain plot.\nWhat package/module might be the best fit for the task.\nAn idea that can be discussed with the larger group for additional thoughts."
  },
  {
    "objectID": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html",
    "href": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html",
    "title": "Grocery Team Week Two Wrap Up",
    "section": "",
    "text": "Gather Data\nStart making basic visualizations\nData Camp\nResearched project"
  },
  {
    "objectID": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html#current-project-objectives",
    "href": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html#current-project-objectives",
    "title": "Grocery Team Week Two Wrap Up",
    "section": "",
    "text": "Gather Data\nStart making basic visualizations\nData Camp\nResearched project"
  },
  {
    "objectID": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html#works-in-progress",
    "href": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html#works-in-progress",
    "title": "Grocery Team Week Two Wrap Up",
    "section": "Works in Progress",
    "text": "Works in Progress\nThis week we practiced using the Tinycensus package, and we looked into data that we found interesting, but the data could be used in our final project.\nThis week we looked at where non-English languages are spoken, which can show if we would need to staff stores with speakers of other languages. We looked at income to see if stores need to prioritize competitive pricing in certain counties."
  },
  {
    "objectID": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html#vacancy-plot-decennial-2020",
    "href": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html#vacancy-plot-decennial-2020",
    "title": "Grocery Team Week Two Wrap Up",
    "section": "Vacancy Plot (Decennial, 2020):",
    "text": "Vacancy Plot (Decennial, 2020):\n\nIowa_Places %&gt;%\n  ggplot(aes(x = NAME)) +\n  geom_col(aes(y = total_percent, fill = \"Occupied\"), width = 0.8) + \n  geom_col(aes(y = percent, fill = \"Vacant\"), width = 0.8) +\n  coord_flip()\n\n\n\n\n\nThis is a graph representing the percentage of vacant housing units per location for both the counties and their respective cities. Slater City has the lowest percentage of vacancies while New Hampton City has the highest."
  },
  {
    "objectID": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html#mode-of-transport-plot-acs5-2021",
    "href": "blog2023/WeekTwo/Grocery-Week-Two/Grocery-Week_Two.html#mode-of-transport-plot-acs5-2021",
    "title": "Grocery Team Week Two Wrap Up",
    "section": "Mode of Transport Plot (ACS5, 2021):",
    "text": "Mode of Transport Plot (ACS5, 2021):\n\nplot_1 &lt;- transport_small %&gt;%\n  ggplot(aes(fill = variable, x = county, y = estimate)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  coord_flip() + \n  ggtitle(\"Mode of Transport to Work\")\n\nplot_2 &lt;- transport_small %&gt;%\n  filter(variable != \"Automobile\") %&gt;%\n  ggplot(aes(fill = variable, x = county, y = estimate)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  coord_flip() +\n  ggtitle(\"Mode of Transport to Work (Other than Automobile)\")\n\nplot_1\n\n\n\n\n\nThis is a plot of the reported mode of transport to work for each of the 4 counties. Commute by automobile overwhelmingly predominates.\n\n\nplot_2\n\n\n\n\n\nThis is a plot of the reported mode of transport for each county with automobile excluded. In two of the counties, “walking” is the majority response."
  },
  {
    "objectID": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html",
    "href": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html",
    "title": "Housing Team Week Two Wrap Up",
    "section": "",
    "text": "Project is more planned out\nPersonal and team Blogs are mostly created\nHalf of backup data is uploaded to GitHub"
  },
  {
    "objectID": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#project-milestones",
    "href": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#project-milestones",
    "title": "Housing Team Week Two Wrap Up",
    "section": "",
    "text": "Project is more planned out\nPersonal and team Blogs are mostly created\nHalf of backup data is uploaded to GitHub"
  },
  {
    "objectID": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#happies",
    "href": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#happies",
    "title": "Housing Team Week Two Wrap Up",
    "section": "Happies",
    "text": "Happies\nWe are in the lead for team datacamp points at about 90k total\nBlogs:\nAngelina\nGavin\nKailyn"
  },
  {
    "objectID": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#crappies",
    "href": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#crappies",
    "title": "Housing Team Week Two Wrap Up",
    "section": "Crappies",
    "text": "Crappies\nThe Blogs are difficult to work with and get set up"
  },
  {
    "objectID": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#cool-technical-things-we-learned-this-week",
    "href": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#cool-technical-things-we-learned-this-week",
    "title": "Housing Team Week Two Wrap Up",
    "section": "Cool Technical Things We Learned This Week",
    "text": "Cool Technical Things We Learned This Week\nHow to created webpages in R studio\nMore basics in R such as making matrices\n\n\n\nMatrices\n\n\nHow to use Tidycensus for importing variable codes\n\n\n\nVariable Loading\n\n\nPractice with GitHub"
  },
  {
    "objectID": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#tidycensus-graphs",
    "href": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#tidycensus-graphs",
    "title": "Housing Team Week Two Wrap Up",
    "section": "Tidycensus Graphs",
    "text": "Tidycensus Graphs\n\n\n\nKailyn\n\n\n\n\n\nGavin\n\n\n\n\n\nKailyn\n\n\n\n\n\nAngelina\n\n\n\n\n\nKailyn\n\n\n\n\n\nGavin"
  },
  {
    "objectID": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#random-facts-for-chris",
    "href": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#random-facts-for-chris",
    "title": "Housing Team Week Two Wrap Up",
    "section": "Random Facts for Chris",
    "text": "Random Facts for Chris\nFrom statistic brain research institute, “…in an average hour, there are over 61,000 Americans airborne over the United States.”\nEvery second, 75 McDonalds burgers are eaten\nIn 1890, the Hollerith Machine was used to tabulate Census data. Technically, this could be called the first computer device."
  },
  {
    "objectID": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#questions-discussion",
    "href": "blog2023/WeekTwo/Housing-Week-Two/Housing-Week-Two.html#questions-discussion",
    "title": "Housing Team Week Two Wrap Up",
    "section": "Questions/ Discussion",
    "text": "Questions/ Discussion\nUnrelated from work - what do people do here during the summer?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Iowa State University DSPG 2023",
    "section": "",
    "text": "This Quarto website is an experimental page hosting project documentation through blogs for the 2023 Data Science for Public Good program at Iowa State University. The Blogs tab contains the documentation for each team’s work on a weekly basis. Week One is omitted since it was individual training and work beyond Week Eight is reflected in the final presentation blogs for each team.\nTo learn more about the program, follow the link to the Iowa State University DSPG Program."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Iowa State University DSPG 2023",
    "section": "",
    "text": "This Quarto website is an experimental page hosting project documentation through blogs for the 2023 Data Science for Public Good program at Iowa State University. The Blogs tab contains the documentation for each team’s work on a weekly basis. Week One is omitted since it was individual training and work beyond Week Eight is reflected in the final presentation blogs for each team.\nTo learn more about the program, follow the link to the Iowa State University DSPG Program."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Iowa State University DSPG 2023",
    "section": "Projects",
    "text": "Projects\n\nData-Driven Insights for Local Food Markets: AI, Pricing, and Crop Flow\nThis project explores artificial intelligence and data analysis tools to benefit local food farms. We developed a web-scraping tool, conducted data analysis, and developed a crop flow optimization model to maximize profit. The tools and insights obtained empower informed decision-making. Moreover, It serves as a precursor to future advancements in demand forecasting and crop\nmanagement for the local food industry.\nProject Sponsor: AI Institute for Resilient Agriculture (AIIRA)\nProject Leads: Rakesh Shah, Lisa Bates\nGraduate Fellows and Interns: Swati Kumari, Mohammad Ahnaf Sadat, Harun Celik, Aaron Case\nFinal Presentation Teaser: YouTube Video\nGitHub: Repository\nProject Blog Pages\nFinal Presentation Video: YouTube\n\n\nAI-Driven Housing Evaluation for Rural Community Development\nThe absence of a comprehensive assessment of housing quality in rural areas of Iowa hinders resource allocation and negatively impacts residents’ well-being and economic growth. To address these challenges, an A1-driven approach is proposed. By utilizing web scraping and AI models, housing features can be categorized as good or poor quality, enabling targeted investment strategies, and directing financial resources where they are most needed. This project aims to reduce biases, streamline housing evaluations, and inform decision-making for rural housing ivestment and development initiatives.\nProject Sponsor: AI Institute for Resilient Agriculture (AIIRA)\nProject Leads: Liesl Eathington, Christopher Seeger\nGraduate Fellows: Morenike Atejioye, Mohammad Ahnaf Sadat\nDSPG Interns: Kailyn Hogan, Angelina Evans, Gavin Fisher\nFinal Presentation Teaser: YouTube Video\nGitHub: Repository\nProject Blog Pages\nFinal Presentation Video: YouTube Video\n\n\nUsing Data to Inform Decision Making for Rural Grocery Stores\nThis DSPG project aims to develop a tool that will help provide users information on opening, inheriting, and operating grocery stores in their preferred rural location. Our work as the DSPGrocery package hosts the calculations used within an R Shiny application to perform dynamic calculations based on user-given inputs for estimating revenues and costs associated with opening a grocery store in a rural location.\nProject Sponsor: Agricultural Marketing Resource Center\nProject Leads: Bailey Hanson, Lisa Bates\nGraduate Fellow: Harun Celik\nDSPG Interns: Aaron Null, Srika Raja, Alex Cory\nFinal Presentation Teaser: YouTube Video\nGitHub: Repository\nProject Blog Pages\nFinal Presentation Video: YouTube"
  }
]